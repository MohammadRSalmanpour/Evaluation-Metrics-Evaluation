{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bibf2B3yeMxe",
        "outputId": "42fdc1e5-1468-48a5-81b8-feee624a9f6d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "79rsLExteN0U",
        "outputId": "2bfd5c87-5eda-4ade-f772-1b668826a838"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics\n",
        "!pip install statsmodels\n",
        "!pip install datasets\n",
        "!pip install SimpleITK\n",
        "!pip install medpy\n",
        "!pip install hausdorff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcLE77bOeRbM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from decimal import getcontext, Decimal\n",
        "\n",
        "np.set_printoptions(precision=25)\n",
        "getcontext().prec = 25\n",
        "pd.options.display.float_format = '{:.25f}'.format  # Adjust the format as needed\n",
        "# Initialize a dictio# Set precision for numpy\n",
        "np.set_printoptions(precision=25)\n",
        "\n",
        "# Set precision for pandas\n",
        "pd.set_option('display.float_format', '{:.25f}'.format)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRQlwBDXgC_8"
      },
      "source": [
        "# Torch 15 pic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xneP_cEgotYQ",
        "outputId": "5e7055c2-b5b6-4a10-8795-eb43aba5e585"
      },
      "outputs": [],
      "source": [
        "# Torch 15 pic\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchmetrics\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nibabel as nib\n",
        "import os\n",
        "from decimal import getcontext, Decimal\n",
        "from torchmetrics.functional import dice\n",
        "\n",
        "np.set_printoptions(precision=25)\n",
        "getcontext().prec = 25\n",
        "pd.options.display.float_format = '{:.25f}'.format  # Adjust the format as needed\n",
        "\n",
        "# Set precision for numpy\n",
        "np.set_printoptions(precision=25)\n",
        "\n",
        "# Set precision for pandas\n",
        "pd.set_option('display.float_format', '{:.25f}'.format)\n",
        "\n",
        "# SegmentationDataset Class\n",
        "# SegmentationDataset Class\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, true_seg_dir, pred_seg_dir, transform=None):\n",
        "        self.true_seg_dir = true_seg_dir\n",
        "        self.pred_seg_dir = pred_seg_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all files in directories\n",
        "        self.true_seg_files = sorted(os.listdir(true_seg_dir))\n",
        "        self.pred_seg_files = sorted(os.listdir(pred_seg_dir))\n",
        "\n",
        "        # Extract the common parts (e.g. #LIDC-IDRI#LIDC-IDRI-0001#VISIT 1#.nii.gz)\n",
        "        self.true_seg_base_names = [self.extract_base_name(file) for file in self.true_seg_files]\n",
        "        self.pred_seg_base_names = [self.extract_base_name(file) for file in self.pred_seg_files]\n",
        "\n",
        "        # Ensure that there are matching filenames between true and predicted\n",
        "        assert set(self.true_seg_base_names) == set(self.pred_seg_base_names), \\\n",
        "            \"True and predicted segmentation filenames do not match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.true_seg_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Find the matching true and predicted segmentation files\n",
        "        base_name = self.true_seg_base_names[idx]\n",
        "        true_seg_filename = [file for file in self.true_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "        pred_seg_filename = [file for file in self.pred_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "\n",
        "        true_seg_path = os.path.join(self.true_seg_dir, true_seg_filename)\n",
        "        pred_seg_path = os.path.join(self.pred_seg_dir, pred_seg_filename)\n",
        "\n",
        "        true_seg_image = nib.load(true_seg_path).get_fdata()\n",
        "        pred_seg_image = nib.load(pred_seg_path).get_fdata()\n",
        "\n",
        "        # Ensure segmentation masks are binary (if needed)\n",
        "        true_seg_image = np.expand_dims(np.clip(true_seg_image, 0, 1), axis=0)\n",
        "        pred_seg_image = np.expand_dims(np.clip(pred_seg_image, 0, 1), axis=0)\n",
        "\n",
        "        sample = {'true_seg': true_seg_image, 'pred_seg': pred_seg_image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def extract_base_name(self, file_name):\n",
        "        base_name = file_name.split('#', 1)[1]\n",
        "        return base_name\n",
        "\n",
        "\n",
        "# ToTensor Class\n",
        "class ToTensor(object):\n",
        "    def __call__(self, sample):\n",
        "        true_seg = torch.from_numpy(sample['true_seg'])\n",
        "        pred_seg = torch.from_numpy(sample['pred_seg'])\n",
        "        true_seg, pred_seg = true_seg.type(torch.float32), pred_seg.type(torch.float32)\n",
        "\n",
        "        x, y, z = 256,256, 128  # Example target shape (D, H, W)\n",
        "        true_seg_resized = F.interpolate(true_seg.unsqueeze(0), size=(x, y, z), mode='trilinear', align_corners=False).squeeze(0)\n",
        "        pred_seg_resized = F.interpolate(pred_seg.unsqueeze(0), size=(x, y, z), mode='trilinear', align_corners=False).squeeze(0)\n",
        "\n",
        "        return {'true_seg': true_seg_resized, 'pred_seg': pred_seg_resized}\n",
        "\n",
        "# Example usage\n",
        "true_segmentations_dir = '/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/true'\n",
        "predicted_masks_dir = '/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/pred'\n",
        "# Create dataset and dataloader\n",
        "dataset = SegmentationDataset(true_seg_dir=true_segmentations_dir, pred_seg_dir=predicted_masks_dir, transform=ToTensor())\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Initialize the results dictionary\n",
        "results2 = {}\n",
        "\n",
        "# Iterate through the dataloader and calculate metrics\n",
        "for i, sample in enumerate(dataloader):\n",
        "    y_true = sample['true_seg']\n",
        "    y_pred = sample['pred_seg']\n",
        "\n",
        "     # Ensure unique samples are being loaded\n",
        "    print(f\"Processing Sample {i}: y_true.shape = {y_true.shape}, y_pred.shape = {y_pred.shape}\")\n",
        "\n",
        "    # Clone and detach the tensors to avoid any computational graph tracking\n",
        "    y_true = y_true.clone().detach().type(torch.float32)\n",
        "    y_pred = y_pred.clone().detach().type(torch.float32)\n",
        "\n",
        "    # Flatten the images for certain metrics\n",
        "    y_true_flat = y_true.flatten()\n",
        "    y_pred_flat = y_pred.flatten()\n",
        "\n",
        "    # PyTorch metrics\n",
        "    y_true_torch = y_true_flat.type(torch.int)\n",
        "    y_pred_torch = y_pred_flat.type(torch.int)\n",
        "\n",
        "    accuracy_metric = torchmetrics.Accuracy(task=\"binary\")\n",
        "    precision_metric = torchmetrics.Precision(task=\"binary\")\n",
        "    recall_metric = torchmetrics.Recall(task=\"binary\")\n",
        "    f1_metric = torchmetrics.F1Score(task=\"binary\")\n",
        "    iou_metric = torchmetrics.JaccardIndex(task=\"binary\")\n",
        "\n",
        "    accuracy_metric.update(y_pred_torch, y_true_torch)\n",
        "    precision_metric.update(y_pred_torch, y_true_torch)\n",
        "    recall_metric.update(y_pred_torch, y_true_torch)\n",
        "    f1_metric.update(y_pred_torch, y_true_torch)\n",
        "    iou_metric.update(y_pred_torch, y_true_torch)\n",
        "\n",
        "    pt_accuracy = accuracy_metric.compute().item()\n",
        "    pt_precision = precision_metric.compute().item()\n",
        "    pt_recall = recall_metric.compute().item()\n",
        "    pt_f1 = f1_metric.compute().item()\n",
        "    pt_iou = iou_metric.compute().item()\n",
        "\n",
        "    # Boundary F1 Score and Hausdorff Distance functions\n",
        "    def extract_boundaries_torch(mask):\n",
        "        \"\"\"Extract boundary pixels from a binary segmentation mask using PyTorch.\"\"\"\n",
        "        # Ensure the mask has 5 dimensions: [batch_size, channels, depth, height, width] for 3D images\n",
        "        if len(mask.shape) == 3:  # If the mask is 3D, expand it\n",
        "            mask = mask.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
        "\n",
        "        # Apply max_pool2d for 2D or 3D pooling (in your case, max_pool3d)\n",
        "        if mask.dim() == 5:  # For 3D images\n",
        "            eroded_mask = F.max_pool3d(mask, kernel_size=3, stride=1, padding=1).squeeze()\n",
        "        else:  # For 2D images (if applicable)\n",
        "            eroded_mask = F.max_pool2d(mask, kernel_size=3, stride=1, padding=1).squeeze()\n",
        "\n",
        "        # Boundary is the difference between the original mask and eroded mask\n",
        "        boundary = (mask.squeeze() != eroded_mask).float()\n",
        "        return boundary\n",
        "\n",
        "\n",
        "    def custom_dilation_torch(mask, kernel_size=3):\n",
        "        \"\"\"Dilate the boundary pixels for 3D data.\"\"\"\n",
        "        kernel = torch.ones((1, 1, kernel_size, kernel_size, kernel_size), dtype=torch.float32)  # 3D kernel\n",
        "        mask = mask.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions if not present\n",
        "\n",
        "        # Use conv3d instead of conv2d since we are dealing with 3D data\n",
        "        dilated = F.conv3d(mask, kernel, padding=kernel_size // 2).squeeze()\n",
        "\n",
        "        dilated = (dilated > 0).float()\n",
        "        return dilated\n",
        "\n",
        "\n",
        "    def boundary_f1_score_torch(y_true, y_pred, dilation_radius=1):\n",
        "        y_true_boundary = extract_boundaries_torch(y_true)\n",
        "        y_pred_boundary = extract_boundaries_torch(y_pred)\n",
        "        y_true_boundary_dilated = custom_dilation_torch(y_true_boundary, kernel_size=2*dilation_radius+1)\n",
        "        y_pred_boundary_dilated = custom_dilation_torch(y_pred_boundary, kernel_size=2*dilation_radius+1)\n",
        "        y_true_boundary_flat = y_true_boundary_dilated.flatten()\n",
        "        y_pred_boundary_flat = y_pred_boundary_dilated.flatten()\n",
        "        precision_metric = torchmetrics.Precision(task=\"binary\")\n",
        "        recall_metric = torchmetrics.Recall(task=\"binary\")\n",
        "        precision_metric.update(y_pred_boundary_flat, y_true_boundary_flat)\n",
        "        recall_metric.update(y_pred_boundary_flat, y_true_boundary_flat)\n",
        "        precision_value = precision_metric.compute().item()\n",
        "        recall_value = recall_metric.compute().item()\n",
        "        f1_value = 2 * (precision_value * recall_value) / (precision_value + recall_value + 1e-6)\n",
        "        return f1_value\n",
        "\n",
        "    bf_score_torch = boundary_f1_score_torch(y_true, y_pred)\n",
        "\n",
        "    def hausdorff_distance_torch(y_true, y_pred):\n",
        "        y_true = torch.tensor(y_true, dtype=torch.float32)\n",
        "        y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
        "        y_true_points = torch.nonzero(y_true, as_tuple=False).float()\n",
        "        y_pred_points = torch.nonzero(y_pred, as_tuple=False).float()\n",
        "        if y_true_points.size(0) == 0 or y_pred_points.size(0) == 0:\n",
        "            return float('inf')\n",
        "        dists = torch.cdist(y_true_points.unsqueeze(0), y_pred_points.unsqueeze(0)).squeeze(0)\n",
        "        forward_hausdorff = torch.max(torch.min(dists, dim=1)[0])\n",
        "        backward_hausdorff = torch.max(torch.min(dists, dim=0)[0])\n",
        "        hd = torch.max(forward_hausdorff, backward_hausdorff)\n",
        "        return hd.item()\n",
        "\n",
        "    hd_torch = hausdorff_distance_torch(y_true, y_pred)\n",
        "\n",
        "    # Save all results\n",
        "    results2[f'Sample {i}'] = {\n",
        "        'Accuracy': pt_accuracy,\n",
        "        'Precision': pt_precision,\n",
        "        'Recall': pt_recall,\n",
        "        'F1 Score': pt_f1,\n",
        "        'IoU': pt_iou,\n",
        "        'Dice Coefficient': dice(y_pred_torch, y_true_torch).item(),\n",
        "        'BF Score': bf_score_torch,\n",
        "        'Hausdorff Distance': hd_torch\n",
        "    }\n",
        "\n",
        "# Convert results to a DataFrame for better readability\n",
        "results2_df = pd.DataFrame(results2).T\n",
        "results2_df.index.name = 'Sample'\n",
        "results2_df.reset_index(inplace=True)\n",
        "print(results2_df)\n",
        "\n",
        "\n",
        "\n",
        "# Save the DataFrame to an Excel file in the 'results' folder\n",
        "results2_df.to_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/binary_TORCH15_metrics.xlsx', index=False)\n",
        "# Torch 15 pic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3IUOxSziRra"
      },
      "source": [
        "# SimpleITK 0-29"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLGf1WOC31rn",
        "outputId": "8dd260d7-70eb-4c2f-8b37-35d5993bbf92"
      },
      "outputs": [],
      "source": [
        "# SimpleITK 0-29\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score, cohen_kappa_score\n",
        "import SimpleITK as sitk\n",
        "from medpy.metric.binary import hd, dc, precision as med_precision, recall as med_recall\n",
        "\n",
        "np.set_printoptions(precision=25)\n",
        "pd.options.display.float_format = '{:.25f}'.format\n",
        "\n",
        "# SegmentationDataset Class (PyTorch)\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, true_seg_dir, pred_seg_dir, transform=None):\n",
        "        self.true_seg_dir = true_seg_dir\n",
        "        self.pred_seg_dir = pred_seg_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all files in directories\n",
        "        self.true_seg_files = sorted(os.listdir(true_seg_dir))\n",
        "        self.pred_seg_files = sorted(os.listdir(pred_seg_dir))\n",
        "\n",
        "        # Extract the common parts (e.g. #LIDC-IDRI#LIDC-IDRI-0001#VISIT 1#.nii.gz)\n",
        "        self.true_seg_base_names = [self.extract_base_name(file) for file in self.true_seg_files]\n",
        "        self.pred_seg_base_names = [self.extract_base_name(file) for file in self.pred_seg_files]\n",
        "\n",
        "        # Ensure that there are matching filenames between true and predicted\n",
        "        assert set(self.true_seg_base_names) == set(self.pred_seg_base_names), \\\n",
        "            \"True and predicted segmentation filenames do not match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.true_seg_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_name = self.true_seg_base_names[idx]\n",
        "        true_seg_filename = [file for file in self.true_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "        pred_seg_filename = [file for file in self.pred_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "\n",
        "        true_seg_path = os.path.join(self.true_seg_dir, true_seg_filename)\n",
        "        pred_seg_path = os.path.join(self.pred_seg_dir, pred_seg_filename)\n",
        "\n",
        "        true_seg_image = nib.load(true_seg_path).get_fdata()\n",
        "        pred_seg_image = nib.load(pred_seg_path).get_fdata()\n",
        "\n",
        "        # Ensure segmentation masks are binary (0 or 1)\n",
        "        true_seg_image = np.expand_dims(np.clip(true_seg_image, 0, 1), axis=0)\n",
        "        pred_seg_image = np.expand_dims(np.clip(pred_seg_image, 0, 1), axis=0)\n",
        "\n",
        "        sample = {'true_seg': true_seg_image, 'pred_seg': pred_seg_image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def extract_base_name(self, file_name):\n",
        "        base_name = file_name.split('#', 1)[1]\n",
        "        return base_name\n",
        "\n",
        "class ToTensorAndResize(object):\n",
        "    def __call__(self, sample):\n",
        "        true_seg = torch.from_numpy(sample['true_seg']).float()\n",
        "        pred_seg = torch.from_numpy(sample['pred_seg']).float()\n",
        "\n",
        "        # Reshape the tensors to target size of 128x128x128 (D, H, W)\n",
        "        true_seg_resized = self.resize_3d(true_seg, (128, 128, 128))\n",
        "        pred_seg_resized = self.resize_3d(pred_seg, (128, 128, 128))\n",
        "\n",
        "        return {'true_seg': true_seg_resized, 'pred_seg': pred_seg_resized}\n",
        "\n",
        "    def resize_3d(self, volume, target_shape):\n",
        "        \"\"\"\n",
        "        Resize a 3D PyTorch tensor to a target shape using trilinear interpolation.\n",
        "        :param volume: Input 3D PyTorch tensor\n",
        "        :param target_shape: Desired shape (D, H, W)\n",
        "        :return: Resized 3D PyTorch tensor\n",
        "        \"\"\"\n",
        "        # Ensure the tensor has at least 3 spatial dimensions\n",
        "        if len(volume.shape) == 3:  # Shape [D, H, W]\n",
        "            volume = volume.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions [1, 1, D, H, W]\n",
        "        elif len(volume.shape) == 4:  # Shape [C, D, H, W]\n",
        "            volume = volume.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Check the shape before interpolation to make sure it's valid\n",
        "        if volume.dim() != 5:\n",
        "            raise ValueError(f\"Expected a 5D tensor, but got {volume.dim()}D tensor with shape {volume.shape}\")\n",
        "\n",
        "        # Perform the interpolation (resizing)\n",
        "        resized_volume = torch.nn.functional.interpolate(volume, size=target_shape, mode='trilinear', align_corners=False)\n",
        "\n",
        "        # Remove the batch and channel dimensions\n",
        "        return resized_volume.squeeze(0).squeeze(0)  # Back to shape [D, H, W]\n",
        "\n",
        "# Load data in PyTorch\n",
        "true_segmentations_dir = '/content/drive/MyDrive/Mr-Alizadeh/Train-760/CT-segmentation'\n",
        "predicted_masks_dir = '/content/drive/MyDrive/Mr-Alizadeh/Reconnet_predicted_masks'\n",
        "\n",
        "# Create dataset (PyTorch)\n",
        "dataset = SegmentationDataset(true_seg_dir=true_segmentations_dir, pred_seg_dir=predicted_masks_dir, transform=ToTensorAndResize())\n",
        "\n",
        "# Initialize the results dictionary\n",
        "results = {}\n",
        "\n",
        "# SimpleITK\n",
        "for i in range(30):\n",
        "    sample = dataset[i]\n",
        "\n",
        "    # Convert PyTorch tensors to NumPy arrays\n",
        "    y_true_np = sample['true_seg'].numpy().astype(np.int32).flatten()\n",
        "    y_pred_np = sample['pred_seg'].numpy().astype(np.int32).flatten()\n",
        "        # بررسی اینکه تصاویر واقعی یا پیش‌بینی‌شده خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        print(f\"True segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        print(f\"Predicted segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "    # بررسی اینکه تصاویر خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        raise ValueError(f\"True segmentation is empty for Sample {i}\")\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        raise ValueError(f\"Predicted segmentation is empty for Sample {i}\")\n",
        "\n",
        "    # بررسی تطابق ابعاد تصاویر\n",
        "    if sample['true_seg'].shape != sample['pred_seg'].shape:\n",
        "        raise ValueError(f\"Shape mismatch: True segmentation shape {sample['true_seg'].shape}, \"\n",
        "                         f\"Predicted segmentation shape {sample['pred_seg'].shape}\")\n",
        "\n",
        "    # Metrics using SimpleITK (Cast to integer type)\n",
        "    true_itk = sitk.Cast(sitk.GetImageFromArray(sample['true_seg'].numpy().astype(np.uint8).squeeze()), sitk.sitkUInt8)\n",
        "    pred_itk = sitk.Cast(sitk.GetImageFromArray(sample['pred_seg'].numpy().astype(np.uint8).squeeze()), sitk.sitkUInt8)\n",
        "\n",
        "    dice_itk = sitk.LabelOverlapMeasuresImageFilter()\n",
        "    dice_itk.Execute(true_itk, pred_itk)\n",
        "    dice_itk = dice_itk.GetDiceCoefficient()\n",
        "    # محاسبه Hausdorff Distance در صورت غیر خالی بودن تصاویر\n",
        "    if np.count_nonzero(y_true_np) > 0 and np.count_nonzero(y_pred_np) > 0:\n",
        "        hd_itk = sitk.HausdorffDistanceImageFilter()\n",
        "        hd_itk.Execute(true_itk, pred_itk)\n",
        "        hd_itk_value = hd_itk.GetHausdorffDistance()\n",
        "    else:\n",
        "        hd_itk_value = np.nan  # یا مقدار پیش‌فرض\n",
        "    # hd_itk = sitk.HausdorffDistanceImageFilter()\n",
        "    # hd_itk.Execute(true_itk, pred_itk)\n",
        "    # hd_itk = hd_itk.GetHausdorffDistance()\n",
        "\n",
        "    # Store the metrics\n",
        "    results[f'SITK_Sample {i}'] = {\n",
        "            'Dice Coefficient': dice_itk,\n",
        "            'Hausdorff Distance': hd_itk\n",
        "    }\n",
        "# Convert results to a DataFrame for better readability\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df.index.name = 'Sample'\n",
        "results_df.reset_index(inplace=True)\n",
        "print(results_df)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "results_df.to_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/binary_SITK_0-29.xlsx', index=False)\n",
        "# SimpleITK 0-29"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1IizPW_7P25"
      },
      "source": [
        "# SimpleITK 30-59"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0UEFUfJ4Cgs",
        "outputId": "6daf7ffa-f2f2-4cb7-becc-871f18d67ed9"
      },
      "outputs": [],
      "source": [
        "# SimpleITK 30-59\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score, cohen_kappa_score\n",
        "import SimpleITK as sitk\n",
        "from medpy.metric.binary import hd, dc, precision as med_precision, recall as med_recall\n",
        "\n",
        "np.set_printoptions(precision=25)\n",
        "pd.options.display.float_format = '{:.25f}'.format\n",
        "\n",
        "# SegmentationDataset Class (PyTorch)\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, true_seg_dir, pred_seg_dir, transform=None):\n",
        "        self.true_seg_dir = true_seg_dir\n",
        "        self.pred_seg_dir = pred_seg_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all files in directories\n",
        "        self.true_seg_files = sorted(os.listdir(true_seg_dir))\n",
        "        self.pred_seg_files = sorted(os.listdir(pred_seg_dir))\n",
        "\n",
        "        # Extract the common parts (e.g. #LIDC-IDRI#LIDC-IDRI-0001#VISIT 1#.nii.gz)\n",
        "        self.true_seg_base_names = [self.extract_base_name(file) for file in self.true_seg_files]\n",
        "        self.pred_seg_base_names = [self.extract_base_name(file) for file in self.pred_seg_files]\n",
        "\n",
        "        # Ensure that there are matching filenames between true and predicted\n",
        "        assert set(self.true_seg_base_names) == set(self.pred_seg_base_names), \\\n",
        "            \"True and predicted segmentation filenames do not match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.true_seg_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_name = self.true_seg_base_names[idx]\n",
        "        true_seg_filename = [file for file in self.true_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "        pred_seg_filename = [file for file in self.pred_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "\n",
        "        true_seg_path = os.path.join(self.true_seg_dir, true_seg_filename)\n",
        "        pred_seg_path = os.path.join(self.pred_seg_dir, pred_seg_filename)\n",
        "\n",
        "        true_seg_image = nib.load(true_seg_path).get_fdata()\n",
        "        pred_seg_image = nib.load(pred_seg_path).get_fdata()\n",
        "\n",
        "        # Ensure segmentation masks are binary (0 or 1)\n",
        "        true_seg_image = np.expand_dims(np.clip(true_seg_image, 0, 1), axis=0)\n",
        "        pred_seg_image = np.expand_dims(np.clip(pred_seg_image, 0, 1), axis=0)\n",
        "\n",
        "        sample = {'true_seg': true_seg_image, 'pred_seg': pred_seg_image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def extract_base_name(self, file_name):\n",
        "        base_name = file_name.split('#', 1)[1]\n",
        "        return base_name\n",
        "\n",
        "class ToTensorAndResize(object):\n",
        "    def __call__(self, sample):\n",
        "        true_seg = torch.from_numpy(sample['true_seg']).float()\n",
        "        pred_seg = torch.from_numpy(sample['pred_seg']).float()\n",
        "\n",
        "        # Reshape the tensors to target size of 128x128x128 (D, H, W)\n",
        "        true_seg_resized = self.resize_3d(true_seg, (128, 128, 128))\n",
        "        pred_seg_resized = self.resize_3d(pred_seg, (128, 128, 128))\n",
        "\n",
        "        return {'true_seg': true_seg_resized, 'pred_seg': pred_seg_resized}\n",
        "\n",
        "    def resize_3d(self, volume, target_shape):\n",
        "        \"\"\"\n",
        "        Resize a 3D PyTorch tensor to a target shape using trilinear interpolation.\n",
        "        :param volume: Input 3D PyTorch tensor\n",
        "        :param target_shape: Desired shape (D, H, W)\n",
        "        :return: Resized 3D PyTorch tensor\n",
        "        \"\"\"\n",
        "        # Ensure the tensor has at least 3 spatial dimensions\n",
        "        if len(volume.shape) == 3:  # Shape [D, H, W]\n",
        "            volume = volume.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions [1, 1, D, H, W]\n",
        "        elif len(volume.shape) == 4:  # Shape [C, D, H, W]\n",
        "            volume = volume.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Check the shape before interpolation to make sure it's valid\n",
        "        if volume.dim() != 5:\n",
        "            raise ValueError(f\"Expected a 5D tensor, but got {volume.dim()}D tensor with shape {volume.shape}\")\n",
        "\n",
        "        # Perform the interpolation (resizing)\n",
        "        resized_volume = torch.nn.functional.interpolate(volume, size=target_shape, mode='trilinear', align_corners=False)\n",
        "\n",
        "        # Remove the batch and channel dimensions\n",
        "        return resized_volume.squeeze(0).squeeze(0)  # Back to shape [D, H, W]\n",
        "\n",
        "# Load data in PyTorch\n",
        "true_segmentations_dir = '/content/drive/MyDrive/Mr-Alizadeh/Train-760/CT-segmentation'\n",
        "predicted_masks_dir = '/content/drive/MyDrive/Mr-Alizadeh/Reconnet_predicted_masks'\n",
        "\n",
        "# Create dataset (PyTorch)\n",
        "dataset = SegmentationDataset(true_seg_dir=true_segmentations_dir, pred_seg_dir=predicted_masks_dir, transform=ToTensorAndResize())\n",
        "\n",
        "# Initialize the results dictionary\n",
        "results = {}\n",
        "\n",
        "# SimpleITK\n",
        "for i in range(30,60):\n",
        "    sample = dataset[i]\n",
        "\n",
        "    # Convert PyTorch tensors to NumPy arrays\n",
        "    y_true_np = sample['true_seg'].numpy().astype(np.int32).flatten()\n",
        "    y_pred_np = sample['pred_seg'].numpy().astype(np.int32).flatten()\n",
        "        # بررسی اینکه تصاویر واقعی یا پیش‌بینی‌شده خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        print(f\"True segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        print(f\"Predicted segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "    # بررسی اینکه تصاویر خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        raise ValueError(f\"True segmentation is empty for Sample {i}\")\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        raise ValueError(f\"Predicted segmentation is empty for Sample {i}\")\n",
        "\n",
        "    # بررسی تطابق ابعاد تصاویر\n",
        "    if sample['true_seg'].shape != sample['pred_seg'].shape:\n",
        "        raise ValueError(f\"Shape mismatch: True segmentation shape {sample['true_seg'].shape}, \"\n",
        "                         f\"Predicted segmentation shape {sample['pred_seg'].shape}\")\n",
        "\n",
        "    # Metrics using SimpleITK (Cast to integer type)\n",
        "    true_itk = sitk.Cast(sitk.GetImageFromArray(sample['true_seg'].numpy().astype(np.uint8).squeeze()), sitk.sitkUInt8)\n",
        "    pred_itk = sitk.Cast(sitk.GetImageFromArray(sample['pred_seg'].numpy().astype(np.uint8).squeeze()), sitk.sitkUInt8)\n",
        "\n",
        "    dice_itk = sitk.LabelOverlapMeasuresImageFilter()\n",
        "    dice_itk.Execute(true_itk, pred_itk)\n",
        "    dice_itk = dice_itk.GetDiceCoefficient()\n",
        "    # محاسبه Hausdorff Distance در صورت غیر خالی بودن تصاویر\n",
        "    if np.count_nonzero(y_true_np) > 0 and np.count_nonzero(y_pred_np) > 0:\n",
        "        hd_itk = sitk.HausdorffDistanceImageFilter()\n",
        "        hd_itk.Execute(true_itk, pred_itk)\n",
        "        hd_itk_value = hd_itk.GetHausdorffDistance()\n",
        "    else:\n",
        "        hd_itk_value = np.nan  # یا مقدار پیش‌فرض\n",
        "    # hd_itk = sitk.HausdorffDistanceImageFilter()\n",
        "    # hd_itk.Execute(true_itk, pred_itk)\n",
        "    # hd_itk = hd_itk.GetHausdorffDistance()\n",
        "\n",
        "    # Store the metrics\n",
        "    results[f'SITK_Sample {i}'] = {\n",
        "            'Dice Coefficient': dice_itk,\n",
        "            'Hausdorff Distance': hd_itk\n",
        "    }\n",
        "# Convert results to a DataFrame for better readability\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df.index.name = 'Sample'\n",
        "results_df.reset_index(inplace=True)\n",
        "print(results_df)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "results_df.to_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/binary_SITK_30-59.xlsx', index=False)\n",
        "# SimpleITK 30-59"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbIMv0x57K2B"
      },
      "source": [
        "# SimpleITK 60-99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PfV2K0o4G_0",
        "outputId": "155336b0-ac57-4ccc-ae24-848d5f3397e9"
      },
      "outputs": [],
      "source": [
        "# SimpleITK 60-99\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import SimpleITK as sitk\n",
        "\n",
        "np.set_printoptions(precision=25)\n",
        "pd.options.display.float_format = '{:.25f}'.format\n",
        "\n",
        "# SegmentationDataset Class (PyTorch)\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, true_seg_dir, pred_seg_dir, transform=None):\n",
        "        self.true_seg_dir = true_seg_dir\n",
        "        self.pred_seg_dir = pred_seg_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all files in directories\n",
        "        self.true_seg_files = sorted(os.listdir(true_seg_dir))\n",
        "        self.pred_seg_files = sorted(os.listdir(pred_seg_dir))\n",
        "\n",
        "        # Extract the common parts (e.g. #LIDC-IDRI#LIDC-IDRI-0001#VISIT 1#.nii.gz)\n",
        "        self.true_seg_base_names = [self.extract_base_name(file) for file in self.true_seg_files]\n",
        "        self.pred_seg_base_names = [self.extract_base_name(file) for file in self.pred_seg_files]\n",
        "\n",
        "        # Ensure that there are matching filenames between true and predicted\n",
        "        assert set(self.true_seg_base_names) == set(self.pred_seg_base_names), \\\n",
        "            \"True and predicted segmentation filenames do not match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.true_seg_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_name = self.true_seg_base_names[idx]\n",
        "        true_seg_filename = [file for file in self.true_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "        pred_seg_filename = [file for file in self.pred_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "\n",
        "        true_seg_path = os.path.join(self.true_seg_dir, true_seg_filename)\n",
        "        pred_seg_path = os.path.join(self.pred_seg_dir, pred_seg_filename)\n",
        "\n",
        "        true_seg_image = nib.load(true_seg_path).get_fdata()\n",
        "        pred_seg_image = nib.load(pred_seg_path).get_fdata()\n",
        "\n",
        "        # Ensure segmentation masks are binary (0 or 1)\n",
        "        true_seg_image = np.expand_dims(np.clip(true_seg_image, 0, 1), axis=0)\n",
        "        pred_seg_image = np.expand_dims(np.clip(pred_seg_image, 0, 1), axis=0)\n",
        "\n",
        "        sample = {'true_seg': true_seg_image, 'pred_seg': pred_seg_image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def extract_base_name(self, file_name):\n",
        "        base_name = file_name.split('#', 1)[1]\n",
        "        return base_name\n",
        "\n",
        "class ToTensorAndResize(object):\n",
        "    def __call__(self, sample):\n",
        "        true_seg = torch.from_numpy(sample['true_seg']).float()\n",
        "        pred_seg = torch.from_numpy(sample['pred_seg']).float()\n",
        "\n",
        "        # Reshape the tensors to target size of 128x128x128 (D, H, W)\n",
        "        true_seg_resized = self.resize_3d(true_seg, (128, 128, 128))\n",
        "        pred_seg_resized = self.resize_3d(pred_seg, (128, 128, 128))\n",
        "\n",
        "        return {'true_seg': true_seg_resized, 'pred_seg': pred_seg_resized}\n",
        "\n",
        "    def resize_3d(self, volume, target_shape):\n",
        "        \"\"\"\n",
        "        Resize a 3D PyTorch tensor to a target shape using trilinear interpolation.\n",
        "        :param volume: Input 3D PyTorch tensor\n",
        "        :param target_shape: Desired shape (D, H, W)\n",
        "        :return: Resized 3D PyTorch tensor\n",
        "        \"\"\"\n",
        "        # Ensure the tensor has at least 3 spatial dimensions\n",
        "        if len(volume.shape) == 3:  # Shape [D, H, W]\n",
        "            volume = volume.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions [1, 1, D, H, W]\n",
        "        elif len(volume.shape) == 4:  # Shape [C, D, H, W]\n",
        "            volume = volume.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Check the shape before interpolation to make sure it's valid\n",
        "        if volume.dim() != 5:\n",
        "            raise ValueError(f\"Expected a 5D tensor, but got {volume.dim()}D tensor with shape {volume.shape}\")\n",
        "\n",
        "        # Perform the interpolation (resizing)\n",
        "        resized_volume = torch.nn.functional.interpolate(volume, size=target_shape, mode='trilinear', align_corners=False)\n",
        "\n",
        "        # Remove the batch and channel dimensions\n",
        "        return resized_volume.squeeze(0).squeeze(0)  # Back to shape [D, H, W]\n",
        "\n",
        "# Load data in PyTorch\n",
        "true_segmentations_dir = '/content/drive/MyDrive/Mr-Alizadeh/Train-760/CT-segmentation'\n",
        "predicted_masks_dir = '/content/drive/MyDrive/Mr-Alizadeh/Reconnet_predicted_masks'\n",
        "\n",
        "# Create dataset (PyTorch)\n",
        "dataset = SegmentationDataset(true_seg_dir=true_segmentations_dir, pred_seg_dir=predicted_masks_dir, transform=ToTensorAndResize())\n",
        "\n",
        "# Initialize the results dictionary\n",
        "results = {}\n",
        "\n",
        "# SimpleITK\n",
        "for i in range(60,100):\n",
        "    sample = dataset[i]\n",
        "\n",
        "    # Convert PyTorch tensors to NumPy arrays\n",
        "    y_true_np = sample['true_seg'].numpy().astype(np.int32).flatten()\n",
        "    y_pred_np = sample['pred_seg'].numpy().astype(np.int32).flatten()\n",
        "        # بررسی اینکه تصاویر واقعی یا پیش‌بینی‌شده خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        print(f\"True segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        print(f\"Predicted segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "    # بررسی اینکه تصاویر خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        raise ValueError(f\"True segmentation is empty for Sample {i}\")\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        raise ValueError(f\"Predicted segmentation is empty for Sample {i}\")\n",
        "\n",
        "    # بررسی تطابق ابعاد تصاویر\n",
        "    if sample['true_seg'].shape != sample['pred_seg'].shape:\n",
        "        raise ValueError(f\"Shape mismatch: True segmentation shape {sample['true_seg'].shape}, \"\n",
        "                         f\"Predicted segmentation shape {sample['pred_seg'].shape}\")\n",
        "\n",
        "    # Metrics using SimpleITK (Cast to integer type)\n",
        "    true_itk = sitk.Cast(sitk.GetImageFromArray(sample['true_seg'].numpy().astype(np.uint8).squeeze()), sitk.sitkUInt8)\n",
        "    pred_itk = sitk.Cast(sitk.GetImageFromArray(sample['pred_seg'].numpy().astype(np.uint8).squeeze()), sitk.sitkUInt8)\n",
        "\n",
        "    dice_itk = sitk.LabelOverlapMeasuresImageFilter()\n",
        "    dice_itk.Execute(true_itk, pred_itk)\n",
        "    dice_itk = dice_itk.GetDiceCoefficient()\n",
        "    # محاسبه Hausdorff Distance در صورت غیر خالی بودن تصاویر\n",
        "    if np.count_nonzero(y_true_np) > 0 and np.count_nonzero(y_pred_np) > 0:\n",
        "        hd_itk = sitk.HausdorffDistanceImageFilter()\n",
        "        hd_itk.Execute(true_itk, pred_itk)\n",
        "        hd_itk_value = hd_itk.GetHausdorffDistance()\n",
        "    else:\n",
        "        hd_itk_value = np.nan  # یا مقدار پیش‌فرض\n",
        "    # hd_itk = sitk.HausdorffDistanceImageFilter()\n",
        "    # hd_itk.Execute(true_itk, pred_itk)\n",
        "    # hd_itk = hd_itk.GetHausdorffDistance()\n",
        "\n",
        "    # Store the metrics\n",
        "    results[f'SITK_Sample {i}'] = {\n",
        "            'Dice Coefficient': dice_itk,\n",
        "            'Hausdorff Distance': hd_itk\n",
        "    }\n",
        "# Convert results to a DataFrame for better readability\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df.index.name = 'Sample'\n",
        "results_df.reset_index(inplace=True)\n",
        "print(results_df)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "results_df.to_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/binary_SITK_60-99.xlsx', index=False)\n",
        "# SimpleITK 60-99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySlkSIyv7Fqx"
      },
      "source": [
        "# Medpy 0-29"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmn38AcA4mn7",
        "outputId": "1bb5c72f-2af0-47b2-ea33-e36faa803feb"
      },
      "outputs": [],
      "source": [
        "# Medpy 0-29\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from medpy.metric.binary import hd, dc, precision as med_precision, recall as med_recall\n",
        "\n",
        "np.set_printoptions(precision=25)\n",
        "pd.options.display.float_format = '{:.25f}'.format\n",
        "\n",
        "# SegmentationDataset Class (PyTorch)\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, true_seg_dir, pred_seg_dir, transform=None):\n",
        "        self.true_seg_dir = true_seg_dir\n",
        "        self.pred_seg_dir = pred_seg_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all files in directories\n",
        "        self.true_seg_files = sorted(os.listdir(true_seg_dir))\n",
        "        self.pred_seg_files = sorted(os.listdir(pred_seg_dir))\n",
        "\n",
        "        # Extract the common parts (e.g. #LIDC-IDRI#LIDC-IDRI-0001#VISIT 1#.nii.gz)\n",
        "        self.true_seg_base_names = [self.extract_base_name(file) for file in self.true_seg_files]\n",
        "        self.pred_seg_base_names = [self.extract_base_name(file) for file in self.pred_seg_files]\n",
        "\n",
        "        # Ensure that there are matching filenames between true and predicted\n",
        "        assert set(self.true_seg_base_names) == set(self.pred_seg_base_names), \\\n",
        "            \"True and predicted segmentation filenames do not match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.true_seg_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_name = self.true_seg_base_names[idx]\n",
        "        true_seg_filename = [file for file in self.true_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "        pred_seg_filename = [file for file in self.pred_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "\n",
        "        true_seg_path = os.path.join(self.true_seg_dir, true_seg_filename)\n",
        "        pred_seg_path = os.path.join(self.pred_seg_dir, pred_seg_filename)\n",
        "\n",
        "        true_seg_image = nib.load(true_seg_path).get_fdata()\n",
        "        pred_seg_image = nib.load(pred_seg_path).get_fdata()\n",
        "\n",
        "        # Ensure segmentation masks are binary (0 or 1)\n",
        "        true_seg_image = np.expand_dims(np.clip(true_seg_image, 0, 1), axis=0)\n",
        "        pred_seg_image = np.expand_dims(np.clip(pred_seg_image, 0, 1), axis=0)\n",
        "\n",
        "        sample = {'true_seg': true_seg_image, 'pred_seg': pred_seg_image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def extract_base_name(self, file_name):\n",
        "        base_name = file_name.split('#', 1)[1]\n",
        "        return base_name\n",
        "\n",
        "class ToTensorAndResize(object):\n",
        "    def __call__(self, sample):\n",
        "        true_seg = torch.from_numpy(sample['true_seg']).float()\n",
        "        pred_seg = torch.from_numpy(sample['pred_seg']).float()\n",
        "\n",
        "        # Reshape the tensors to target size of 128x128x128 (D, H, W)\n",
        "        true_seg_resized = self.resize_3d(true_seg, (128, 128, 128))\n",
        "        pred_seg_resized = self.resize_3d(pred_seg, (128, 128, 128))\n",
        "\n",
        "        return {'true_seg': true_seg_resized, 'pred_seg': pred_seg_resized}\n",
        "\n",
        "    def resize_3d(self, volume, target_shape):\n",
        "        \"\"\"\n",
        "        Resize a 3D PyTorch tensor to a target shape using trilinear interpolation.\n",
        "        :param volume: Input 3D PyTorch tensor\n",
        "        :param target_shape: Desired shape (D, H, W)\n",
        "        :return: Resized 3D PyTorch tensor\n",
        "        \"\"\"\n",
        "        # Ensure the tensor has at least 3 spatial dimensions\n",
        "        if len(volume.shape) == 3:  # Shape [D, H, W]\n",
        "            volume = volume.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions [1, 1, D, H, W]\n",
        "        elif len(volume.shape) == 4:  # Shape [C, D, H, W]\n",
        "            volume = volume.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Check the shape before interpolation to make sure it's valid\n",
        "        if volume.dim() != 5:\n",
        "            raise ValueError(f\"Expected a 5D tensor, but got {volume.dim()}D tensor with shape {volume.shape}\")\n",
        "\n",
        "        # Perform the interpolation (resizing)\n",
        "        resized_volume = torch.nn.functional.interpolate(volume, size=target_shape, mode='trilinear', align_corners=False)\n",
        "\n",
        "        # Remove the batch and channel dimensions\n",
        "        return resized_volume.squeeze(0).squeeze(0)  # Back to shape [D, H, W]\n",
        "\n",
        "# Load data in PyTorch\n",
        "true_segmentations_dir = '/content/drive/MyDrive/Mr-Alizadeh/Train-760/CT-segmentation'\n",
        "predicted_masks_dir = '/content/drive/MyDrive/Mr-Alizadeh/Reconnet_predicted_masks'\n",
        "# Create dataset (PyTorch)\n",
        "dataset = SegmentationDataset(true_seg_dir=true_segmentations_dir, pred_seg_dir=predicted_masks_dir, transform=ToTensorAndResize())\n",
        "\n",
        "# Initialize the results dictionary\n",
        "results = {}\n",
        "\n",
        "# Iterate through the dataset and calculate metrics using Scikit-learn, SimpleITK, and MedPy\n",
        "for i in range(30):\n",
        "    sample = dataset[i]\n",
        "\n",
        "    # Convert PyTorch tensors to NumPy arrays\n",
        "    y_true_np = sample['true_seg'].numpy().astype(np.int32).flatten()\n",
        "    y_pred_np = sample['pred_seg'].numpy().astype(np.int32).flatten()\n",
        "        # بررسی اینکه تصاویر واقعی یا پیش‌بینی‌شده خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        print(f\"True segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        print(f\"Predicted segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "    # بررسی اینکه تصاویر خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        raise ValueError(f\"True segmentation is empty for Sample {i}\")\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        raise ValueError(f\"Predicted segmentation is empty for Sample {i}\")\n",
        "\n",
        "    # بررسی تطابق ابعاد تصاویر\n",
        "    if sample['true_seg'].shape != sample['pred_seg'].shape:\n",
        "        raise ValueError(f\"Shape mismatch: True segmentation shape {sample['true_seg'].shape}, \"\n",
        "                         f\"Predicted segmentation shape {sample['pred_seg'].shape}\")\n",
        "\n",
        "\n",
        "    # Metrics using MedPy\n",
        "    dc_medpy = dc(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "    hausdorff_distance_med = hd(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "    precision_medpy = med_precision(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "    recall_medpy = med_recall(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "\n",
        "    # Manually calculate IoU for MedPy (Jaccard Index)\n",
        "    tp = np.sum((y_true_np == 1) & (y_pred_np == 1))\n",
        "    fp = np.sum((y_true_np == 0) & (y_pred_np == 1))\n",
        "    fn = np.sum((y_true_np == 1) & (y_pred_np == 0))\n",
        "    iou_medpy = tp / (tp + fp + fn + 1e-6)  # Add a small epsilon to avoid division by zero\n",
        "\n",
        "    # Calculate F1 Score for MedPy\n",
        "    f1_medpy = 2 * (precision_medpy * recall_medpy) / (precision_medpy + recall_medpy + 1e-6)\n",
        "\n",
        "    # Store the metrics\n",
        "    results[f'Med_Sample {i}'] = {\n",
        "            'Dice Coefficient': dc_medpy,\n",
        "            'Hausdorff Distance': hausdorff_distance_med,\n",
        "            'IoU': iou_medpy,\n",
        "            'Precision': precision_medpy,\n",
        "            'Recall': recall_medpy,\n",
        "            'F1 Score': f1_medpy\n",
        "    }\n",
        "\n",
        "# Convert results to a DataFrame for better readability\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df.index.name = 'Sample'\n",
        "results_df.reset_index(inplace=True)\n",
        "print(results_df)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "results_df.to_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/binary_Med_0-29.xlsx', index=False)\n",
        "# Medpy 0-29"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIdi2ocP7Aw6"
      },
      "source": [
        "# Medpy 30-59"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8qM5B6f4422",
        "outputId": "fc96db9b-1a8e-4194-9323-1350f57f13de"
      },
      "outputs": [],
      "source": [
        "# Medpy 30-59\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from medpy.metric.binary import hd, dc, precision as med_precision, recall as med_recall\n",
        "\n",
        "np.set_printoptions(precision=25)\n",
        "pd.options.display.float_format = '{:.25f}'.format\n",
        "\n",
        "# SegmentationDataset Class (PyTorch)\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, true_seg_dir, pred_seg_dir, transform=None):\n",
        "        self.true_seg_dir = true_seg_dir\n",
        "        self.pred_seg_dir = pred_seg_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all files in directories\n",
        "        self.true_seg_files = sorted(os.listdir(true_seg_dir))\n",
        "        self.pred_seg_files = sorted(os.listdir(pred_seg_dir))\n",
        "\n",
        "        # Extract the common parts (e.g. #LIDC-IDRI#LIDC-IDRI-0001#VISIT 1#.nii.gz)\n",
        "        self.true_seg_base_names = [self.extract_base_name(file) for file in self.true_seg_files]\n",
        "        self.pred_seg_base_names = [self.extract_base_name(file) for file in self.pred_seg_files]\n",
        "\n",
        "        # Ensure that there are matching filenames between true and predicted\n",
        "        assert set(self.true_seg_base_names) == set(self.pred_seg_base_names), \\\n",
        "            \"True and predicted segmentation filenames do not match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.true_seg_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_name = self.true_seg_base_names[idx]\n",
        "        true_seg_filename = [file for file in self.true_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "        pred_seg_filename = [file for file in self.pred_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "\n",
        "        true_seg_path = os.path.join(self.true_seg_dir, true_seg_filename)\n",
        "        pred_seg_path = os.path.join(self.pred_seg_dir, pred_seg_filename)\n",
        "\n",
        "        true_seg_image = nib.load(true_seg_path).get_fdata()\n",
        "        pred_seg_image = nib.load(pred_seg_path).get_fdata()\n",
        "\n",
        "        # Ensure segmentation masks are binary (0 or 1)\n",
        "        true_seg_image = np.expand_dims(np.clip(true_seg_image, 0, 1), axis=0)\n",
        "        pred_seg_image = np.expand_dims(np.clip(pred_seg_image, 0, 1), axis=0)\n",
        "\n",
        "        sample = {'true_seg': true_seg_image, 'pred_seg': pred_seg_image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def extract_base_name(self, file_name):\n",
        "        base_name = file_name.split('#', 1)[1]\n",
        "        return base_name\n",
        "\n",
        "class ToTensorAndResize(object):\n",
        "    def __call__(self, sample):\n",
        "        true_seg = torch.from_numpy(sample['true_seg']).float()\n",
        "        pred_seg = torch.from_numpy(sample['pred_seg']).float()\n",
        "\n",
        "        # Reshape the tensors to target size of 128x128x128 (D, H, W)\n",
        "        true_seg_resized = self.resize_3d(true_seg, (128, 128, 128))\n",
        "        pred_seg_resized = self.resize_3d(pred_seg, (128, 128, 128))\n",
        "\n",
        "        return {'true_seg': true_seg_resized, 'pred_seg': pred_seg_resized}\n",
        "\n",
        "    def resize_3d(self, volume, target_shape):\n",
        "        \"\"\"\n",
        "        Resize a 3D PyTorch tensor to a target shape using trilinear interpolation.\n",
        "        :param volume: Input 3D PyTorch tensor\n",
        "        :param target_shape: Desired shape (D, H, W)\n",
        "        :return: Resized 3D PyTorch tensor\n",
        "        \"\"\"\n",
        "        # Ensure the tensor has at least 3 spatial dimensions\n",
        "        if len(volume.shape) == 3:  # Shape [D, H, W]\n",
        "            volume = volume.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions [1, 1, D, H, W]\n",
        "        elif len(volume.shape) == 4:  # Shape [C, D, H, W]\n",
        "            volume = volume.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Check the shape before interpolation to make sure it's valid\n",
        "        if volume.dim() != 5:\n",
        "            raise ValueError(f\"Expected a 5D tensor, but got {volume.dim()}D tensor with shape {volume.shape}\")\n",
        "\n",
        "        # Perform the interpolation (resizing)\n",
        "        resized_volume = torch.nn.functional.interpolate(volume, size=target_shape, mode='trilinear', align_corners=False)\n",
        "\n",
        "        # Remove the batch and channel dimensions\n",
        "        return resized_volume.squeeze(0).squeeze(0)  # Back to shape [D, H, W]\n",
        "\n",
        "# Load data in PyTorch\n",
        "true_segmentations_dir = '/content/drive/MyDrive/Mr-Alizadeh/Train-760/CT-segmentation'\n",
        "predicted_masks_dir = '/content/drive/MyDrive/Mr-Alizadeh/Reconnet_predicted_masks'\n",
        "# Create dataset (PyTorch)\n",
        "dataset = SegmentationDataset(true_seg_dir=true_segmentations_dir, pred_seg_dir=predicted_masks_dir, transform=ToTensorAndResize())\n",
        "\n",
        "# Initialize the results dictionary\n",
        "results = {}\n",
        "\n",
        "# Iterate through the dataset and calculate metrics using Scikit-learn, SimpleITK, and MedPy\n",
        "for i in range(30,60):\n",
        "    sample = dataset[i]\n",
        "\n",
        "    # Convert PyTorch tensors to NumPy arrays\n",
        "    y_true_np = sample['true_seg'].numpy().astype(np.int32).flatten()\n",
        "    y_pred_np = sample['pred_seg'].numpy().astype(np.int32).flatten()\n",
        "        # بررسی اینکه تصاویر واقعی یا پیش‌بینی‌شده خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        print(f\"True segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        print(f\"Predicted segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "    # بررسی اینکه تصاویر خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        raise ValueError(f\"True segmentation is empty for Sample {i}\")\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        raise ValueError(f\"Predicted segmentation is empty for Sample {i}\")\n",
        "\n",
        "    # بررسی تطابق ابعاد تصاویر\n",
        "    if sample['true_seg'].shape != sample['pred_seg'].shape:\n",
        "        raise ValueError(f\"Shape mismatch: True segmentation shape {sample['true_seg'].shape}, \"\n",
        "                         f\"Predicted segmentation shape {sample['pred_seg'].shape}\")\n",
        "\n",
        "\n",
        "    # Metrics using MedPy\n",
        "    dc_medpy = dc(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "    hausdorff_distance_med = hd(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "    precision_medpy = med_precision(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "    recall_medpy = med_recall(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "\n",
        "    # Manually calculate IoU for MedPy (Jaccard Index)\n",
        "    tp = np.sum((y_true_np == 1) & (y_pred_np == 1))\n",
        "    fp = np.sum((y_true_np == 0) & (y_pred_np == 1))\n",
        "    fn = np.sum((y_true_np == 1) & (y_pred_np == 0))\n",
        "    iou_medpy = tp / (tp + fp + fn + 1e-6)  # Add a small epsilon to avoid division by zero\n",
        "\n",
        "    # Calculate F1 Score for MedPy\n",
        "    f1_medpy = 2 * (precision_medpy * recall_medpy) / (precision_medpy + recall_medpy + 1e-6)\n",
        "\n",
        "    # Store the metrics\n",
        "    results[f'Med_Sample {i}'] = {\n",
        "            'Dice Coefficient': dc_medpy,\n",
        "            'Hausdorff Distance': hausdorff_distance_med,\n",
        "            'IoU': iou_medpy,\n",
        "            'Precision': precision_medpy,\n",
        "            'Recall': recall_medpy,\n",
        "            'F1 Score': f1_medpy\n",
        "    }\n",
        "\n",
        "# Convert results to a DataFrame for better readability\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df.index.name = 'Sample'\n",
        "results_df.reset_index(inplace=True)\n",
        "print(results_df)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "results_df.to_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/binary_Med_30-59.xlsx', index=False)\n",
        "# Medpy 30-59"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaPcio1y6576"
      },
      "source": [
        "# Medpy 60-99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Alic2sMV5E3D",
        "outputId": "80d56612-c54e-45cd-a0f1-a85ec8931f65"
      },
      "outputs": [],
      "source": [
        "# Medpy 60-99\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from medpy.metric.binary import hd, dc, precision as med_precision, recall as med_recall\n",
        "\n",
        "np.set_printoptions(precision=25)\n",
        "pd.options.display.float_format = '{:.25f}'.format\n",
        "\n",
        "# SegmentationDataset Class (PyTorch)\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, true_seg_dir, pred_seg_dir, transform=None):\n",
        "        self.true_seg_dir = true_seg_dir\n",
        "        self.pred_seg_dir = pred_seg_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all files in directories\n",
        "        self.true_seg_files = sorted(os.listdir(true_seg_dir))\n",
        "        self.pred_seg_files = sorted(os.listdir(pred_seg_dir))\n",
        "\n",
        "        # Extract the common parts (e.g. #LIDC-IDRI#LIDC-IDRI-0001#VISIT 1#.nii.gz)\n",
        "        self.true_seg_base_names = [self.extract_base_name(file) for file in self.true_seg_files]\n",
        "        self.pred_seg_base_names = [self.extract_base_name(file) for file in self.pred_seg_files]\n",
        "\n",
        "        # Ensure that there are matching filenames between true and predicted\n",
        "        assert set(self.true_seg_base_names) == set(self.pred_seg_base_names), \\\n",
        "            \"True and predicted segmentation filenames do not match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.true_seg_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_name = self.true_seg_base_names[idx]\n",
        "        true_seg_filename = [file for file in self.true_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "        pred_seg_filename = [file for file in self.pred_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "\n",
        "        true_seg_path = os.path.join(self.true_seg_dir, true_seg_filename)\n",
        "        pred_seg_path = os.path.join(self.pred_seg_dir, pred_seg_filename)\n",
        "\n",
        "        true_seg_image = nib.load(true_seg_path).get_fdata()\n",
        "        pred_seg_image = nib.load(pred_seg_path).get_fdata()\n",
        "\n",
        "        # Ensure segmentation masks are binary (0 or 1)\n",
        "        true_seg_image = np.expand_dims(np.clip(true_seg_image, 0, 1), axis=0)\n",
        "        pred_seg_image = np.expand_dims(np.clip(pred_seg_image, 0, 1), axis=0)\n",
        "\n",
        "        sample = {'true_seg': true_seg_image, 'pred_seg': pred_seg_image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def extract_base_name(self, file_name):\n",
        "        base_name = file_name.split('#', 1)[1]\n",
        "        return base_name\n",
        "\n",
        "class ToTensorAndResize(object):\n",
        "    def __call__(self, sample):\n",
        "        true_seg = torch.from_numpy(sample['true_seg']).float()\n",
        "        pred_seg = torch.from_numpy(sample['pred_seg']).float()\n",
        "\n",
        "        # Reshape the tensors to target size of 128x128x128 (D, H, W)\n",
        "        true_seg_resized = self.resize_3d(true_seg, (128, 128, 128))\n",
        "        pred_seg_resized = self.resize_3d(pred_seg, (128, 128, 128))\n",
        "\n",
        "        return {'true_seg': true_seg_resized, 'pred_seg': pred_seg_resized}\n",
        "\n",
        "    def resize_3d(self, volume, target_shape):\n",
        "        \"\"\"\n",
        "        Resize a 3D PyTorch tensor to a target shape using trilinear interpolation.\n",
        "        :param volume: Input 3D PyTorch tensor\n",
        "        :param target_shape: Desired shape (D, H, W)\n",
        "        :return: Resized 3D PyTorch tensor\n",
        "        \"\"\"\n",
        "        # Ensure the tensor has at least 3 spatial dimensions\n",
        "        if len(volume.shape) == 3:  # Shape [D, H, W]\n",
        "            volume = volume.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions [1, 1, D, H, W]\n",
        "        elif len(volume.shape) == 4:  # Shape [C, D, H, W]\n",
        "            volume = volume.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Check the shape before interpolation to make sure it's valid\n",
        "        if volume.dim() != 5:\n",
        "            raise ValueError(f\"Expected a 5D tensor, but got {volume.dim()}D tensor with shape {volume.shape}\")\n",
        "\n",
        "        # Perform the interpolation (resizing)\n",
        "        resized_volume = torch.nn.functional.interpolate(volume, size=target_shape, mode='trilinear', align_corners=False)\n",
        "\n",
        "        # Remove the batch and channel dimensions\n",
        "        return resized_volume.squeeze(0).squeeze(0)  # Back to shape [D, H, W]\n",
        "\n",
        "# Load data in PyTorch\n",
        "true_segmentations_dir = '/content/drive/MyDrive/Mr-Alizadeh/Train-760/CT-segmentation'\n",
        "predicted_masks_dir = '/content/drive/MyDrive/Mr-Alizadeh/Reconnet_predicted_masks'\n",
        "# Create dataset (PyTorch)\n",
        "dataset = SegmentationDataset(true_seg_dir=true_segmentations_dir, pred_seg_dir=predicted_masks_dir, transform=ToTensorAndResize())\n",
        "\n",
        "# Initialize the results dictionary\n",
        "results = {}\n",
        "\n",
        "# Iterate through the dataset and calculate metrics using Scikit-learn, SimpleITK, and MedPy\n",
        "for i in range(60,100):\n",
        "    sample = dataset[i]\n",
        "\n",
        "    # Convert PyTorch tensors to NumPy arrays\n",
        "    y_true_np = sample['true_seg'].numpy().astype(np.int32).flatten()\n",
        "    y_pred_np = sample['pred_seg'].numpy().astype(np.int32).flatten()\n",
        "        # بررسی اینکه تصاویر واقعی یا پیش‌بینی‌شده خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        print(f\"True segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        print(f\"Predicted segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "    # بررسی اینکه تصاویر خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        raise ValueError(f\"True segmentation is empty for Sample {i}\")\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        raise ValueError(f\"Predicted segmentation is empty for Sample {i}\")\n",
        "\n",
        "    # بررسی تطابق ابعاد تصاویر\n",
        "    if sample['true_seg'].shape != sample['pred_seg'].shape:\n",
        "        raise ValueError(f\"Shape mismatch: True segmentation shape {sample['true_seg'].shape}, \"\n",
        "                         f\"Predicted segmentation shape {sample['pred_seg'].shape}\")\n",
        "\n",
        "\n",
        "    # Metrics using MedPy\n",
        "    dc_medpy = dc(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "    hausdorff_distance_med = hd(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "    precision_medpy = med_precision(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "    recall_medpy = med_recall(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "\n",
        "    # Manually calculate IoU for MedPy (Jaccard Index)\n",
        "    tp = np.sum((y_true_np == 1) & (y_pred_np == 1))\n",
        "    fp = np.sum((y_true_np == 0) & (y_pred_np == 1))\n",
        "    fn = np.sum((y_true_np == 1) & (y_pred_np == 0))\n",
        "    iou_medpy = tp / (tp + fp + fn + 1e-6)  # Add a small epsilon to avoid division by zero\n",
        "\n",
        "    # Calculate F1 Score for MedPy\n",
        "    f1_medpy = 2 * (precision_medpy * recall_medpy) / (precision_medpy + recall_medpy + 1e-6)\n",
        "\n",
        "    # Store the metrics\n",
        "    results[f'Med_Sample {i}'] = {\n",
        "            'Dice Coefficient': dc_medpy,\n",
        "            'Hausdorff Distance': hausdorff_distance_med,\n",
        "            'IoU': iou_medpy,\n",
        "            'Precision': precision_medpy,\n",
        "            'Recall': recall_medpy,\n",
        "            'F1 Score': f1_medpy\n",
        "    }\n",
        "\n",
        "# Convert results to a DataFrame for better readability\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df.index.name = 'Sample'\n",
        "results_df.reset_index(inplace=True)\n",
        "print(results_df)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "results_df.to_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/binary_Med_60-99.xlsx', index=False)\n",
        "# Medpy 60-99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDA8oCUj6xT0"
      },
      "source": [
        "# 2D binary Medpy 2 pic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqtZrr_J6Lex",
        "outputId": "1c8c6e8e-105e-4538-e61a-edba2f0b696a"
      },
      "outputs": [],
      "source": [
        "# 2D Medpy 2 pic\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from medpy.metric.binary import hd, dc, precision as med_precision, recall as med_recall\n",
        "\n",
        "np.set_printoptions(precision=25)\n",
        "pd.options.display.float_format = '{:.25f}'.format\n",
        "\n",
        "# SegmentationDataset Class (PyTorch)\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, true_seg_dir, pred_seg_dir, transform=None):\n",
        "        self.true_seg_dir = true_seg_dir\n",
        "        self.pred_seg_dir = pred_seg_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all files in directories\n",
        "        self.true_seg_files = sorted(os.listdir(true_seg_dir))\n",
        "        self.pred_seg_files = sorted(os.listdir(pred_seg_dir))\n",
        "\n",
        "        # Extract the common parts (e.g. #LIDC-IDRI#LIDC-IDRI-0001#VISIT 1#.nii.gz)\n",
        "        self.true_seg_base_names = [self.extract_base_name(file) for file in self.true_seg_files]\n",
        "        self.pred_seg_base_names = [self.extract_base_name(file) for file in self.pred_seg_files]\n",
        "\n",
        "        # Ensure that there are matching filenames between true and predicted\n",
        "        assert set(self.true_seg_base_names) == set(self.pred_seg_base_names), \\\n",
        "            \"True and predicted segmentation filenames do not match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.true_seg_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_name = self.true_seg_base_names[idx]\n",
        "        true_seg_filename = [file for file in self.true_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "        pred_seg_filename = [file for file in self.pred_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "\n",
        "        true_seg_path = os.path.join(self.true_seg_dir, true_seg_filename)\n",
        "        pred_seg_path = os.path.join(self.pred_seg_dir, pred_seg_filename)\n",
        "\n",
        "        true_seg_image = nib.load(true_seg_path).get_fdata()\n",
        "        pred_seg_image = nib.load(pred_seg_path).get_fdata()\n",
        "\n",
        "        # Ensure segmentation masks are binary (0 or 1)\n",
        "        true_seg_image = np.expand_dims(np.clip(true_seg_image, 0, 1), axis=0)\n",
        "        pred_seg_image = np.expand_dims(np.clip(pred_seg_image, 0, 1), axis=0)\n",
        "\n",
        "        sample = {'true_seg': true_seg_image, 'pred_seg': pred_seg_image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def extract_base_name(self, file_name):\n",
        "        base_name = file_name.split('#', 1)[1]\n",
        "        return base_name\n",
        "\n",
        "class ToTensorAndResize(object):\n",
        "    def __call__(self, sample):\n",
        "        true_seg = torch.from_numpy(sample['true_seg']).float()\n",
        "        pred_seg = torch.from_numpy(sample['pred_seg']).float()\n",
        "\n",
        "        # Reshape the tensors to target size of 128x128x128 (D, H, W)\n",
        "        true_seg_resized = self.resize_3d(true_seg, (128, 128, 128))\n",
        "        pred_seg_resized = self.resize_3d(pred_seg, (128, 128, 128))\n",
        "\n",
        "        return {'true_seg': true_seg_resized, 'pred_seg': pred_seg_resized}\n",
        "\n",
        "    def resize_3d(self, volume, target_shape):\n",
        "        \"\"\"\n",
        "        Resize a 3D PyTorch tensor to a target shape using trilinear interpolation.\n",
        "        :param volume: Input 3D PyTorch tensor\n",
        "        :param target_shape: Desired shape (D, H, W)\n",
        "        :return: Resized 3D PyTorch tensor\n",
        "        \"\"\"\n",
        "        # Ensure the tensor has at least 3 spatial dimensions\n",
        "        if len(volume.shape) == 3:  # Shape [D, H, W]\n",
        "            volume = volume.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions [1, 1, D, H, W]\n",
        "        elif len(volume.shape) == 4:  # Shape [C, D, H, W]\n",
        "            volume = volume.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Check the shape before interpolation to make sure it's valid\n",
        "        if volume.dim() != 5:\n",
        "            raise ValueError(f\"Expected a 5D tensor, but got {volume.dim()}D tensor with shape {volume.shape}\")\n",
        "\n",
        "        # Perform the interpolation (resizing)\n",
        "        resized_volume = torch.nn.functional.interpolate(volume, size=target_shape, mode='trilinear', align_corners=False)\n",
        "\n",
        "        # Remove the batch and channel dimensions\n",
        "        return resized_volume.squeeze(0).squeeze(0)  # Back to shape [D, H, W]\n",
        "\n",
        "# Load data in PyTorch\n",
        "true_segmentations_dir = '/content/drive/MyDrive/Saba/true_seg'\n",
        "predicted_masks_dir = '/content/drive/MyDrive/Saba/pred_seg'\n",
        "\n",
        "# Create dataset (PyTorch)\n",
        "dataset = SegmentationDataset(true_seg_dir=true_segmentations_dir, pred_seg_dir=predicted_masks_dir, transform=ToTensorAndResize())\n",
        "\n",
        "# Initialize the results dictionary\n",
        "results = {}\n",
        "\n",
        "# Iterate through the dataset and calculate metrics using Scikit-learn, SimpleITK, and MedPy\n",
        "for i in range(len(dataset)):\n",
        "    sample = dataset[i]\n",
        "\n",
        "    # Convert PyTorch tensors to NumPy arrays\n",
        "    y_true_np = sample['true_seg'].numpy().astype(np.int32).flatten()\n",
        "    y_pred_np = sample['pred_seg'].numpy().astype(np.int32).flatten()\n",
        "        # بررسی اینکه تصاویر واقعی یا پیش‌بینی‌شده خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        print(f\"True segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        print(f\"Predicted segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "    # بررسی اینکه تصاویر خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        raise ValueError(f\"True segmentation is empty for Sample {i}\")\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        raise ValueError(f\"Predicted segmentation is empty for Sample {i}\")\n",
        "\n",
        "    # بررسی تطابق ابعاد تصاویر\n",
        "    if sample['true_seg'].shape != sample['pred_seg'].shape:\n",
        "        raise ValueError(f\"Shape mismatch: True segmentation shape {sample['true_seg'].shape}, \"\n",
        "                         f\"Predicted segmentation shape {sample['pred_seg'].shape}\")\n",
        "\n",
        "\n",
        "    # Metrics using MedPy\n",
        "    dc_medpy = dc(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "    hausdorff_distance_med = hd(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "    precision_medpy = med_precision(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "    recall_medpy = med_recall(sample['pred_seg'].numpy().astype(np.int32).flatten(), sample['true_seg'].numpy().astype(np.int32).flatten())\n",
        "\n",
        "    # Manually calculate IoU for MedPy (Jaccard Index)\n",
        "    tp = np.sum((y_true_np == 1) & (y_pred_np == 1))\n",
        "    fp = np.sum((y_true_np == 0) & (y_pred_np == 1))\n",
        "    fn = np.sum((y_true_np == 1) & (y_pred_np == 0))\n",
        "    iou_medpy = tp / (tp + fp + fn + 1e-6)  # Add a small epsilon to avoid division by zero\n",
        "\n",
        "    # Calculate F1 Score for MedPy\n",
        "    f1_medpy = 2 * (precision_medpy * recall_medpy) / (precision_medpy + recall_medpy + 1e-6)\n",
        "\n",
        "    # Store the metrics\n",
        "    results[f'Med_Sample {i}'] = {\n",
        "            'Dice Coefficient': dc_medpy,\n",
        "            'Hausdorff Distance': hausdorff_distance_med,\n",
        "            'IoU': iou_medpy,\n",
        "            'Precision': precision_medpy,\n",
        "            'Recall': recall_medpy,\n",
        "            'F1 Score': f1_medpy\n",
        "    }\n",
        "\n",
        "# Convert results to a DataFrame for better readability\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df.index.name = 'Sample'\n",
        "results_df.reset_index(inplace=True)\n",
        "print(results_df)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "results_df.to_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/2D_binary_Med_2pic.xlsx', index=False)\n",
        "# 2D Medpy 2 pic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ekCKa0J3m1F"
      },
      "source": [
        "# SL 760"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aAQjS_eAiiIJ",
        "outputId": "266b464f-19fd-4e7d-9369-3fb504ef1a22"
      },
      "outputs": [],
      "source": [
        "# SL 760\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score, cohen_kappa_score\n",
        "import SimpleITK as sitk\n",
        "from medpy.metric.binary import hd, dc, precision as med_precision, recall as med_recall\n",
        "\n",
        "np.set_printoptions(precision=25)\n",
        "pd.options.display.float_format = '{:.25f}'.format\n",
        "\n",
        "# SegmentationDataset Class (PyTorch)\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, true_seg_dir, pred_seg_dir, transform=None):\n",
        "        self.true_seg_dir = true_seg_dir\n",
        "        self.pred_seg_dir = pred_seg_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all files in directories\n",
        "        self.true_seg_files = sorted(os.listdir(true_seg_dir))\n",
        "        self.pred_seg_files = sorted(os.listdir(pred_seg_dir))\n",
        "\n",
        "        # Extract the common parts (e.g. #LIDC-IDRI#LIDC-IDRI-0001#VISIT 1#.nii.gz)\n",
        "        self.true_seg_base_names = [self.extract_base_name(file) for file in self.true_seg_files]\n",
        "        self.pred_seg_base_names = [self.extract_base_name(file) for file in self.pred_seg_files]\n",
        "\n",
        "        # Ensure that there are matching filenames between true and predicted\n",
        "        assert set(self.true_seg_base_names) == set(self.pred_seg_base_names), \\\n",
        "            \"True and predicted segmentation filenames do not match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.true_seg_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_name = self.true_seg_base_names[idx]\n",
        "        true_seg_filename = [file for file in self.true_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "        pred_seg_filename = [file for file in self.pred_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "\n",
        "        true_seg_path = os.path.join(self.true_seg_dir, true_seg_filename)\n",
        "        pred_seg_path = os.path.join(self.pred_seg_dir, pred_seg_filename)\n",
        "\n",
        "        true_seg_image = nib.load(true_seg_path).get_fdata()\n",
        "        pred_seg_image = nib.load(pred_seg_path).get_fdata()\n",
        "\n",
        "        # Ensure segmentation masks are binary (0 or 1)\n",
        "        true_seg_image = np.expand_dims(np.clip(true_seg_image, 0, 1), axis=0)\n",
        "        pred_seg_image = np.expand_dims(np.clip(pred_seg_image, 0, 1), axis=0)\n",
        "\n",
        "        sample = {'true_seg': true_seg_image, 'pred_seg': pred_seg_image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def extract_base_name(self, file_name):\n",
        "        base_name = file_name.split('#', 1)[1]\n",
        "        return base_name\n",
        "\n",
        "class ToTensorAndResize(object):\n",
        "    def __call__(self, sample):\n",
        "        true_seg = torch.from_numpy(sample['true_seg']).float()\n",
        "        pred_seg = torch.from_numpy(sample['pred_seg']).float()\n",
        "\n",
        "        # Reshape the tensors to target size of 128x128x128 (D, H, W)\n",
        "        true_seg_resized = self.resize_3d(true_seg, (128, 128, 128))\n",
        "        pred_seg_resized = self.resize_3d(pred_seg, (128, 128, 128))\n",
        "\n",
        "        return {'true_seg': true_seg_resized, 'pred_seg': pred_seg_resized}\n",
        "\n",
        "    def resize_3d(self, volume, target_shape):\n",
        "        \"\"\"\n",
        "        Resize a 3D PyTorch tensor to a target shape using trilinear interpolation.\n",
        "        :param volume: Input 3D PyTorch tensor\n",
        "        :param target_shape: Desired shape (D, H, W)\n",
        "        :return: Resized 3D PyTorch tensor\n",
        "        \"\"\"\n",
        "        # Ensure the tensor has at least 3 spatial dimensions\n",
        "        if len(volume.shape) == 3:  # Shape [D, H, W]\n",
        "            volume = volume.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions [1, 1, D, H, W]\n",
        "        elif len(volume.shape) == 4:  # Shape [C, D, H, W]\n",
        "            volume = volume.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Check the shape before interpolation to make sure it's valid\n",
        "        if volume.dim() != 5:\n",
        "            raise ValueError(f\"Expected a 5D tensor, but got {volume.dim()}D tensor with shape {volume.shape}\")\n",
        "\n",
        "        # Perform the interpolation (resizing)\n",
        "        resized_volume = torch.nn.functional.interpolate(volume, size=target_shape, mode='trilinear', align_corners=False)\n",
        "\n",
        "        # Remove the batch and channel dimensions\n",
        "        return resized_volume.squeeze(0).squeeze(0)  # Back to shape [D, H, W]\n",
        "\n",
        "# Load data in PyTorch\n",
        "true_segmentations_dir = '/content/drive/MyDrive/Mr-Alizadeh/Train-760/CT-segmentation'\n",
        "predicted_masks_dir = '/content/drive/MyDrive/Mr-Alizadeh/Reconnet_predicted_masks'\n",
        "\n",
        "# Create dataset (PyTorch)\n",
        "dataset = SegmentationDataset(true_seg_dir=true_segmentations_dir, pred_seg_dir=predicted_masks_dir, transform=ToTensorAndResize())\n",
        "\n",
        "# Initialize the results dictionary\n",
        "results = {}\n",
        "\n",
        "# Iterate through the dataset and calculate metrics using Scikit-learn, SimpleITK, and MedPy\n",
        "for i in range(len(dataset)):\n",
        "    sample = dataset[i]\n",
        "\n",
        "    # Convert PyTorch tensors to NumPy arrays\n",
        "    y_true_np = sample['true_seg'].numpy().astype(np.int32).flatten()\n",
        "    y_pred_np = sample['pred_seg'].numpy().astype(np.int32).flatten()\n",
        "        # بررسی اینکه تصاویر واقعی یا پیش‌بینی‌شده خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        print(f\"True segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        print(f\"Predicted segmentation is empty for Sample {i}, setting metrics to NaN.\")\n",
        "        sklearn_accuracy = np.nan\n",
        "        sklearn_precision = np.nan\n",
        "        sklearn_recall = np.nan\n",
        "        # سایر متریک‌ها را با NaN پر کنید...\n",
        "        continue\n",
        "    # بررسی اینکه تصاویر خالی نیستند\n",
        "    if np.count_nonzero(y_true_np) == 0:\n",
        "        raise ValueError(f\"True segmentation is empty for Sample {i}\")\n",
        "\n",
        "    if np.count_nonzero(y_pred_np) == 0:\n",
        "        raise ValueError(f\"Predicted segmentation is empty for Sample {i}\")\n",
        "\n",
        "    # بررسی تطابق ابعاد تصاویر\n",
        "    if sample['true_seg'].shape != sample['pred_seg'].shape:\n",
        "        raise ValueError(f\"Shape mismatch: True segmentation shape {sample['true_seg'].shape}, \"\n",
        "                         f\"Predicted segmentation shape {sample['pred_seg'].shape}\")\n",
        "\n",
        "    # Metrics using Scikit-learn\n",
        "    sklearn_accuracy = accuracy_score(y_true_np, y_pred_np)\n",
        "    sklearn_precision = precision_score(y_true_np, y_pred_np)\n",
        "    sklearn_recall = recall_score(y_true_np, y_pred_np)\n",
        "    sklearn_f1 = f1_score(y_true_np, y_pred_np)\n",
        "    sklearn_jaccard = jaccard_score(y_true_np, y_pred_np)\n",
        "    sklearn_kappa = cohen_kappa_score(y_true_np, y_pred_np)\n",
        "\n",
        "    # Boundary F1 Score (calculated manually or using Scikit-learn's precision/recall)\n",
        "    bf_score_skl = 2 * (sklearn_precision * sklearn_recall) / (sklearn_precision + sklearn_recall + 1e-6)\n",
        "\n",
        "\n",
        "    # Store the metrics\n",
        "    results[f'Sk_Sample {i}'] = {\n",
        "            'Accuracy': sklearn_accuracy,\n",
        "            'Precision': sklearn_precision,\n",
        "            'Recall': sklearn_recall,\n",
        "            'F1 Score': sklearn_f1,\n",
        "            'IoU': sklearn_jaccard,\n",
        "            'Kappa': sklearn_kappa,\n",
        "            'BF Score': bf_score_skl\n",
        "        }\n",
        "# Convert results to a DataFrame for better readability\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df.index.name = 'Sample'\n",
        "results_df.reset_index(inplace=True)\n",
        "print(results_df)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "results_df.to_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/binary_SL_0-29.xlsx', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsZoiibYf4dr"
      },
      "source": [
        "# TF 15 pic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ju4GmALjrK1p",
        "outputId": "437e1b63-5ee0-4f45-aa29-0f45a4e8011f"
      },
      "outputs": [],
      "source": [
        "# TF 10 pic\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from torch.utils.data import Dataset\n",
        "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy, MeanIoU\n",
        "\n",
        "np.set_printoptions(precision=25)\n",
        "pd.options.display.float_format = '{:.25f}'.format\n",
        "\n",
        "# SegmentationDataset Class (PyTorch)\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, true_seg_dir, pred_seg_dir, transform=None):\n",
        "        self.true_seg_dir = true_seg_dir\n",
        "        self.pred_seg_dir = pred_seg_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all files in directories\n",
        "        self.true_seg_files = sorted(os.listdir(true_seg_dir))\n",
        "        self.pred_seg_files = sorted(os.listdir(pred_seg_dir))\n",
        "\n",
        "        # Extract the common parts (e.g. #LIDC-IDRI#LIDC-IDRI-0001#VISIT 1#.nii.gz)\n",
        "        self.true_seg_base_names = [self.extract_base_name(file) for file in self.true_seg_files]\n",
        "        self.pred_seg_base_names = [self.extract_base_name(file) for file in self.pred_seg_files]\n",
        "\n",
        "        # Ensure that there are matching filenames between true and predicted\n",
        "        assert set(self.true_seg_base_names) == set(self.pred_seg_base_names), \\\n",
        "            \"True and predicted segmentation filenames do not match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.true_seg_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_name = self.true_seg_base_names[idx]\n",
        "        true_seg_filename = [file for file in self.true_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "        pred_seg_filename = [file for file in self.pred_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "\n",
        "        true_seg_path = os.path.join(self.true_seg_dir, true_seg_filename)\n",
        "        pred_seg_path = os.path.join(self.pred_seg_dir, pred_seg_filename)\n",
        "\n",
        "        true_seg_image = nib.load(true_seg_path).get_fdata()\n",
        "        pred_seg_image = nib.load(pred_seg_path).get_fdata()\n",
        "\n",
        "        # Ensure segmentation masks are binary (0 or 1)\n",
        "        true_seg_image = np.expand_dims(np.clip(true_seg_image, 0, 1), axis=0)\n",
        "        pred_seg_image = np.expand_dims(np.clip(pred_seg_image, 0, 1), axis=0)\n",
        "\n",
        "        sample = {'true_seg': true_seg_image, 'pred_seg': pred_seg_image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def extract_base_name(self, file_name):\n",
        "        base_name = file_name.split('#', 1)[1]\n",
        "        return base_name\n",
        "\n",
        "class ToTensorAndResize(object):\n",
        "    def __call__(self, sample):\n",
        "        true_seg = torch.from_numpy(sample['true_seg']).float()\n",
        "        pred_seg = torch.from_numpy(sample['pred_seg']).float()\n",
        "\n",
        "        # Reshape the tensors to target size of 128x128x128 (D, H, W)\n",
        "        true_seg_resized = self.resize_3d(true_seg, (128, 128, 128))\n",
        "        pred_seg_resized = self.resize_3d(pred_seg, (128, 128, 128))\n",
        "\n",
        "        return {'true_seg': true_seg_resized, 'pred_seg': pred_seg_resized}\n",
        "\n",
        "    def resize_3d(self, volume, target_shape):\n",
        "        \"\"\"\n",
        "        Resize a 3D PyTorch tensor to a target shape using trilinear interpolation.\n",
        "        :param volume: Input 3D PyTorch tensor\n",
        "        :param target_shape: Desired shape (D, H, W)\n",
        "        :return: Resized 3D PyTorch tensor\n",
        "        \"\"\"\n",
        "        # Ensure the tensor has at least 3 spatial dimensions\n",
        "        if len(volume.shape) == 3:  # Shape [D, H, W]\n",
        "            volume = volume.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions [1, 1, D, H, W]\n",
        "        elif len(volume.shape) == 4:  # Shape [C, D, H, W]\n",
        "            volume = volume.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Check the shape before interpolation to make sure it's valid\n",
        "        if volume.dim() != 5:\n",
        "            raise ValueError(f\"Expected a 5D tensor, but got {volume.dim()}D tensor with shape {volume.shape}\")\n",
        "\n",
        "        # Perform the interpolation (resizing)\n",
        "        resized_volume = torch.nn.functional.interpolate(volume, size=target_shape, mode='trilinear', align_corners=False)\n",
        "\n",
        "        # Remove the batch and channel dimensions\n",
        "        return resized_volume.squeeze(0).squeeze(0)  # Back to shape [D, H, W]\n",
        "\n",
        "\n",
        "# Load data in PyTorch\n",
        "true_segmentations_dir = '/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/true'\n",
        "predicted_masks_dir = '/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/pred'\n",
        "\n",
        "# Create dataset (PyTorch)\n",
        "dataset = SegmentationDataset(true_seg_dir=true_segmentations_dir, pred_seg_dir=predicted_masks_dir, transform=ToTensorAndResize())\n",
        "\n",
        "# Initialize the results dictionary for TensorFlow metrics\n",
        "results_tf = {}\n",
        "\n",
        "# Iterate through the dataset and calculate metrics using TensorFlow\n",
        "for i in range(len(dataset)):\n",
        "    sample = dataset[i]\n",
        "\n",
        "    # Convert PyTorch tensors to NumPy and then TensorFlow tensors\n",
        "    y_true_np = sample['true_seg'].numpy()\n",
        "    y_pred_np = sample['pred_seg'].numpy()\n",
        "\n",
        "    y_true_tf = tf.convert_to_tensor(y_true_np, dtype=tf.float32)\n",
        "    y_pred_tf = tf.convert_to_tensor(y_pred_np, dtype=tf.float32)\n",
        "\n",
        "    # Flatten the tensors for certain metrics\n",
        "    y_true_flat = tf.reshape(y_true_tf, [-1])\n",
        "    y_pred_flat = tf.reshape(y_pred_tf, [-1])\n",
        "\n",
        "    # Convert to binary format for metrics\n",
        "    y_true_bin = tf.cast(y_true_flat, dtype=tf.int32)\n",
        "    y_pred_bin = tf.cast(y_pred_flat, dtype=tf.int32)\n",
        "\n",
        "    # TensorFlow/Keras metrics\n",
        "    accuracy = BinaryAccuracy()\n",
        "    precision = Precision()\n",
        "    recall = Recall()\n",
        "    iou = MeanIoU(num_classes=2)\n",
        "\n",
        "    # Update and compute the metrics\n",
        "    accuracy.update_state(y_true_bin, y_pred_bin)\n",
        "    precision.update_state(y_true_bin, y_pred_bin)\n",
        "    recall.update_state(y_true_bin, y_pred_bin)\n",
        "    iou.update_state(y_true_bin, y_pred_bin)\n",
        "\n",
        "    # Calculate boundary F1 and Hausdorff Distance\n",
        "    def extract_boundaries_tf(mask):\n",
        "        eroded_mask = tf.nn.max_pool3d(mask[None, None, ...], ksize=3, strides=1, padding='SAME')[0, 0]\n",
        "        boundary = tf.cast(tf.not_equal(mask, eroded_mask), dtype=tf.float32)\n",
        "        return boundary\n",
        "\n",
        "    def boundary_f1_score_tf(y_true, y_pred, dilation_radius=1):\n",
        "        y_true_boundary = extract_boundaries_tf(y_true)\n",
        "        y_pred_boundary = extract_boundaries_tf(y_pred)\n",
        "        y_true_boundary_flat = tf.reshape(y_true_boundary, [-1])\n",
        "        y_pred_boundary_flat = tf.reshape(y_pred_boundary, [-1])\n",
        "        precision_value = tf.reduce_sum(y_true_boundary_flat * y_pred_boundary_flat) / (tf.reduce_sum(y_pred_boundary_flat) + 1e-6)\n",
        "        recall_value = tf.reduce_sum(y_true_boundary_flat * y_pred_boundary_flat) / (tf.reduce_sum(y_true_boundary_flat) + 1e-6)\n",
        "        bf_score = 2 * (precision_value * recall_value) / (precision_value + recall_value + 1e-6)\n",
        "        return bf_score.numpy()\n",
        "\n",
        "    def hausdorff_distance_tf(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate the Hausdorff Distance between two 3D binary masks.\n",
        "        :param y_true: Ground truth mask\n",
        "        :param y_pred: Predicted mask\n",
        "        :return: Hausdorff Distance\n",
        "        \"\"\"\n",
        "        # Get the indices of the true and predicted points\n",
        "        y_true_points = tf.where(tf.equal(y_true, 1))\n",
        "        y_pred_points = tf.where(tf.equal(y_pred, 1))\n",
        "\n",
        "        # Cast the points to float32 for distance calculation\n",
        "        y_true_points = tf.cast(y_true_points, dtype=tf.float32)\n",
        "        y_pred_points = tf.cast(y_pred_points, dtype=tf.float32)\n",
        "\n",
        "        # Handle cases where there are no points in either y_true or y_pred\n",
        "        if tf.shape(y_true_points)[0] == 0 or tf.shape(y_pred_points)[0] == 0:\n",
        "            return float('inf')  # Return infinity if there are no points to compare\n",
        "\n",
        "        # Calculate pairwise distances between points in y_true and y_pred\n",
        "        dists = tf.norm(tf.expand_dims(y_true_points, 1) - tf.expand_dims(y_pred_points, 0), axis=-1)\n",
        "\n",
        "        # Hausdorff distance is the maximum of the minimum distances from y_true to y_pred and vice versa\n",
        "        forward_hd = tf.reduce_max(tf.reduce_min(dists, axis=1))\n",
        "        backward_hd = tf.reduce_max(tf.reduce_min(dists, axis=0))\n",
        "\n",
        "        # Return the Hausdorff distance\n",
        "        return tf.reduce_max([forward_hd, backward_hd]).numpy()\n",
        "\n",
        "\n",
        "    # Calculate boundary F1 and Hausdorff Distance\n",
        "    bf_score_tf = boundary_f1_score_tf(y_true_tf, y_pred_tf)\n",
        "    hd_tf = hausdorff_distance_tf(y_true_tf, y_pred_tf)\n",
        "\n",
        "    # Store results\n",
        "    results_tf[f'Sample {i}'] = {\n",
        "        'Accuracy': accuracy.result().numpy(),\n",
        "        'Precision': precision.result().numpy(),\n",
        "        'Recall': recall.result().numpy(),\n",
        "        'IoU': iou.result().numpy(),\n",
        "        'BF Score': bf_score_tf,\n",
        "        'Hausdorff Distance': hd_tf\n",
        "    }\n",
        "\n",
        "# Convert results to a DataFrame for better readability\n",
        "results_tf_df = pd.DataFrame(results_tf).T\n",
        "results_tf_df.index.name = 'Sample'\n",
        "results_tf_df.reset_index(inplace=True)\n",
        "print(results_tf_df)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "# os.makedirs('./results', exist_ok=True)\n",
        "results_tf_df.to_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/binary_TF15_metrics.xlsx', index=False)\n",
        "# Torch 15 pic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE1zIhkvfeHc"
      },
      "source": [
        "# TF 0-29"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AOfuUC2XYgxr",
        "outputId": "f27b5b4a-ca75-4a4c-80ad-bbdbba544081"
      },
      "outputs": [],
      "source": [
        "# TF 30\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from torch.utils.data import Dataset\n",
        "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy, MeanIoU\n",
        "\n",
        "np.set_printoptions(precision=25)\n",
        "pd.options.display.float_format = '{:.25f}'.format\n",
        "\n",
        "# SegmentationDataset Class (PyTorch)\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, true_seg_dir, pred_seg_dir, transform=None):\n",
        "        self.true_seg_dir = true_seg_dir\n",
        "        self.pred_seg_dir = pred_seg_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all files in directories\n",
        "        self.true_seg_files = sorted(os.listdir(true_seg_dir))\n",
        "        self.pred_seg_files = sorted(os.listdir(pred_seg_dir))\n",
        "\n",
        "        # Extract the common parts (e.g. #LIDC-IDRI#LIDC-IDRI-0001#VISIT 1#.nii.gz)\n",
        "        self.true_seg_base_names = [self.extract_base_name(file) for file in self.true_seg_files]\n",
        "        self.pred_seg_base_names = [self.extract_base_name(file) for file in self.pred_seg_files]\n",
        "\n",
        "        # Ensure that there are matching filenames between true and predicted\n",
        "        assert set(self.true_seg_base_names) == set(self.pred_seg_base_names), \\\n",
        "            \"True and predicted segmentation filenames do not match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.true_seg_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_name = self.true_seg_base_names[idx]\n",
        "        true_seg_filename = [file for file in self.true_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "        pred_seg_filename = [file for file in self.pred_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "\n",
        "        true_seg_path = os.path.join(self.true_seg_dir, true_seg_filename)\n",
        "        pred_seg_path = os.path.join(self.pred_seg_dir, pred_seg_filename)\n",
        "\n",
        "        true_seg_image = nib.load(true_seg_path).get_fdata()\n",
        "        pred_seg_image = nib.load(pred_seg_path).get_fdata()\n",
        "\n",
        "        # Ensure segmentation masks are binary (0 or 1)\n",
        "        true_seg_image = np.expand_dims(np.clip(true_seg_image, 0, 1), axis=0)\n",
        "        pred_seg_image = np.expand_dims(np.clip(pred_seg_image, 0, 1), axis=0)\n",
        "\n",
        "        sample = {'true_seg': true_seg_image, 'pred_seg': pred_seg_image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def extract_base_name(self, file_name):\n",
        "        base_name = file_name.split('#', 1)[1]\n",
        "        return base_name\n",
        "\n",
        "class ToTensorAndResize(object):\n",
        "    def __call__(self, sample):\n",
        "        true_seg = torch.from_numpy(sample['true_seg']).float()\n",
        "        pred_seg = torch.from_numpy(sample['pred_seg']).float()\n",
        "\n",
        "        # Reshape the tensors to target size of 128x128x128 (D, H, W)\n",
        "        true_seg_resized = self.resize_3d(true_seg, (128, 128, 128))\n",
        "        pred_seg_resized = self.resize_3d(pred_seg, (128, 128, 128))\n",
        "\n",
        "        return {'true_seg': true_seg_resized, 'pred_seg': pred_seg_resized}\n",
        "\n",
        "    def resize_3d(self, volume, target_shape):\n",
        "        \"\"\"\n",
        "        Resize a 3D PyTorch tensor to a target shape using trilinear interpolation.\n",
        "        :param volume: Input 3D PyTorch tensor\n",
        "        :param target_shape: Desired shape (D, H, W)\n",
        "        :return: Resized 3D PyTorch tensor\n",
        "        \"\"\"\n",
        "        # Ensure the tensor has at least 3 spatial dimensions\n",
        "        if len(volume.shape) == 3:  # Shape [D, H, W]\n",
        "            volume = volume.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions [1, 1, D, H, W]\n",
        "        elif len(volume.shape) == 4:  # Shape [C, D, H, W]\n",
        "            volume = volume.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Check the shape before interpolation to make sure it's valid\n",
        "        if volume.dim() != 5:\n",
        "            raise ValueError(f\"Expected a 5D tensor, but got {volume.dim()}D tensor with shape {volume.shape}\")\n",
        "\n",
        "        # Perform the interpolation (resizing)\n",
        "        resized_volume = torch.nn.functional.interpolate(volume, size=target_shape, mode='trilinear', align_corners=False)\n",
        "\n",
        "        # Remove the batch and channel dimensions\n",
        "        return resized_volume.squeeze(0).squeeze(0)  # Back to shape [D, H, W]\n",
        "\n",
        "\n",
        "# Load data in PyTorch\n",
        "true_segmentations_dir = '/content/drive/MyDrive/Mr-Alizadeh/Train-760/CT-segmentation'\n",
        "predicted_masks_dir = '/content/drive/MyDrive/Mr-Alizadeh/Reconnet_predicted_masks'\n",
        "\n",
        "# Create dataset (PyTorch)\n",
        "dataset = SegmentationDataset(true_seg_dir=true_segmentations_dir, pred_seg_dir=predicted_masks_dir, transform=ToTensorAndResize())\n",
        "\n",
        "# Initialize the results dictionary for TensorFlow metrics\n",
        "results_tf = {}\n",
        "\n",
        "# Iterate through the dataset and calculate metrics using TensorFlow\n",
        "for i in range(30):\n",
        "    sample = dataset[i]\n",
        "\n",
        "    # Convert PyTorch tensors to NumPy and then TensorFlow tensors\n",
        "    y_true_np = sample['true_seg'].numpy()\n",
        "    y_pred_np = sample['pred_seg'].numpy()\n",
        "\n",
        "    y_true_tf = tf.convert_to_tensor(y_true_np, dtype=tf.float32)\n",
        "    y_pred_tf = tf.convert_to_tensor(y_pred_np, dtype=tf.float32)\n",
        "\n",
        "    # Flatten the tensors for certain metrics\n",
        "    y_true_flat = tf.reshape(y_true_tf, [-1])\n",
        "    y_pred_flat = tf.reshape(y_pred_tf, [-1])\n",
        "\n",
        "    # Convert to binary format for metrics\n",
        "    y_true_bin = tf.cast(y_true_flat, dtype=tf.int32)\n",
        "    y_pred_bin = tf.cast(y_pred_flat, dtype=tf.int32)\n",
        "\n",
        "    # TensorFlow/Keras metrics\n",
        "    accuracy = BinaryAccuracy()\n",
        "    precision = Precision()\n",
        "    recall = Recall()\n",
        "    iou = MeanIoU(num_classes=2)\n",
        "\n",
        "    # Update and compute the metrics\n",
        "    accuracy.update_state(y_true_bin, y_pred_bin)\n",
        "    precision.update_state(y_true_bin, y_pred_bin)\n",
        "    recall.update_state(y_true_bin, y_pred_bin)\n",
        "    iou.update_state(y_true_bin, y_pred_bin)\n",
        "\n",
        "    # Calculate boundary F1 and Hausdorff Distance\n",
        "    def extract_boundaries_tf(mask):\n",
        "        eroded_mask = tf.nn.max_pool3d(mask[None, None, ...], ksize=3, strides=1, padding='SAME')[0, 0]\n",
        "        boundary = tf.cast(tf.not_equal(mask, eroded_mask), dtype=tf.float32)\n",
        "        return boundary\n",
        "\n",
        "    def boundary_f1_score_tf(y_true, y_pred, dilation_radius=1):\n",
        "        y_true_boundary = extract_boundaries_tf(y_true)\n",
        "        y_pred_boundary = extract_boundaries_tf(y_pred)\n",
        "        y_true_boundary_flat = tf.reshape(y_true_boundary, [-1])\n",
        "        y_pred_boundary_flat = tf.reshape(y_pred_boundary, [-1])\n",
        "        precision_value = tf.reduce_sum(y_true_boundary_flat * y_pred_boundary_flat) / (tf.reduce_sum(y_pred_boundary_flat) + 1e-6)\n",
        "        recall_value = tf.reduce_sum(y_true_boundary_flat * y_pred_boundary_flat) / (tf.reduce_sum(y_true_boundary_flat) + 1e-6)\n",
        "        bf_score = 2 * (precision_value * recall_value) / (precision_value + recall_value + 1e-6)\n",
        "        return bf_score.numpy()\n",
        "\n",
        "    def hausdorff_distance_tf(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate the Hausdorff Distance between two 3D binary masks.\n",
        "        :param y_true: Ground truth mask\n",
        "        :param y_pred: Predicted mask\n",
        "        :return: Hausdorff Distance\n",
        "        \"\"\"\n",
        "        # Get the indices of the true and predicted points\n",
        "        y_true_points = tf.where(tf.equal(y_true, 1))\n",
        "        y_pred_points = tf.where(tf.equal(y_pred, 1))\n",
        "\n",
        "        # Cast the points to float32 for distance calculation\n",
        "        y_true_points = tf.cast(y_true_points, dtype=tf.float32)\n",
        "        y_pred_points = tf.cast(y_pred_points, dtype=tf.float32)\n",
        "\n",
        "        # Handle cases where there are no points in either y_true or y_pred\n",
        "        if tf.shape(y_true_points)[0] == 0 or tf.shape(y_pred_points)[0] == 0:\n",
        "            return float('inf')  # Return infinity if there are no points to compare\n",
        "\n",
        "        # Calculate pairwise distances between points in y_true and y_pred\n",
        "        dists = tf.norm(tf.expand_dims(y_true_points, 1) - tf.expand_dims(y_pred_points, 0), axis=-1)\n",
        "\n",
        "        # Hausdorff distance is the maximum of the minimum distances from y_true to y_pred and vice versa\n",
        "        forward_hd = tf.reduce_max(tf.reduce_min(dists, axis=1))\n",
        "        backward_hd = tf.reduce_max(tf.reduce_min(dists, axis=0))\n",
        "\n",
        "        # Return the Hausdorff distance\n",
        "        return tf.reduce_max([forward_hd, backward_hd]).numpy()\n",
        "\n",
        "\n",
        "    # Calculate boundary F1 and Hausdorff Distance\n",
        "    bf_score_tf = boundary_f1_score_tf(y_true_tf, y_pred_tf)\n",
        "    hd_tf = hausdorff_distance_tf(y_true_tf, y_pred_tf)\n",
        "\n",
        "    # Store results\n",
        "    results_tf[f'Sample {i}'] = {\n",
        "        'Accuracy': accuracy.result().numpy(),\n",
        "        'Precision': precision.result().numpy(),\n",
        "        'Recall': recall.result().numpy(),\n",
        "        'IoU': iou.result().numpy(),\n",
        "        'BF Score': bf_score_tf,\n",
        "        'Hausdorff Distance': hd_tf\n",
        "    }\n",
        "\n",
        "# Convert results to a DataFrame for better readability\n",
        "results_tf_df = pd.DataFrame(results_tf).T\n",
        "results_tf_df.index.name = 'Sample'\n",
        "results_tf_df.reset_index(inplace=True)\n",
        "print(results_tf_df)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "# os.makedirs('./results', exist_ok=True)\n",
        "results_tf_df.to_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/binary_TF30.xlsx', index=False)\n",
        "# Torch 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-NyZUQ8fQh_"
      },
      "source": [
        "# TF 30-59"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HkhY17uVdMBO",
        "outputId": "cf980485-cb79-41f2-b493-3816e395e87f"
      },
      "outputs": [],
      "source": [
        "# TF 30-59\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from torch.utils.data import Dataset\n",
        "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy, MeanIoU\n",
        "\n",
        "np.set_printoptions(precision=25)\n",
        "pd.options.display.float_format = '{:.25f}'.format\n",
        "\n",
        "# SegmentationDataset Class (PyTorch)\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, true_seg_dir, pred_seg_dir, transform=None):\n",
        "        self.true_seg_dir = true_seg_dir\n",
        "        self.pred_seg_dir = pred_seg_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all files in directories\n",
        "        self.true_seg_files = sorted(os.listdir(true_seg_dir))\n",
        "        self.pred_seg_files = sorted(os.listdir(pred_seg_dir))\n",
        "\n",
        "        # Extract the common parts (e.g. #LIDC-IDRI#LIDC-IDRI-0001#VISIT 1#.nii.gz)\n",
        "        self.true_seg_base_names = [self.extract_base_name(file) for file in self.true_seg_files]\n",
        "        self.pred_seg_base_names = [self.extract_base_name(file) for file in self.pred_seg_files]\n",
        "\n",
        "        # Ensure that there are matching filenames between true and predicted\n",
        "        assert set(self.true_seg_base_names) == set(self.pred_seg_base_names), \\\n",
        "            \"True and predicted segmentation filenames do not match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.true_seg_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_name = self.true_seg_base_names[idx]\n",
        "        true_seg_filename = [file for file in self.true_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "        pred_seg_filename = [file for file in self.pred_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "\n",
        "        true_seg_path = os.path.join(self.true_seg_dir, true_seg_filename)\n",
        "        pred_seg_path = os.path.join(self.pred_seg_dir, pred_seg_filename)\n",
        "\n",
        "        true_seg_image = nib.load(true_seg_path).get_fdata()\n",
        "        pred_seg_image = nib.load(pred_seg_path).get_fdata()\n",
        "\n",
        "        # Ensure segmentation masks are binary (0 or 1)\n",
        "        true_seg_image = np.expand_dims(np.clip(true_seg_image, 0, 1), axis=0)\n",
        "        pred_seg_image = np.expand_dims(np.clip(pred_seg_image, 0, 1), axis=0)\n",
        "\n",
        "        sample = {'true_seg': true_seg_image, 'pred_seg': pred_seg_image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def extract_base_name(self, file_name):\n",
        "        base_name = file_name.split('#', 1)[1]\n",
        "        return base_name\n",
        "\n",
        "class ToTensorAndResize(object):\n",
        "    def __call__(self, sample):\n",
        "        true_seg = torch.from_numpy(sample['true_seg']).float()\n",
        "        pred_seg = torch.from_numpy(sample['pred_seg']).float()\n",
        "\n",
        "        # Reshape the tensors to target size of 128x128x128 (D, H, W)\n",
        "        true_seg_resized = self.resize_3d(true_seg, (128, 128, 128))\n",
        "        pred_seg_resized = self.resize_3d(pred_seg, (128, 128, 128))\n",
        "\n",
        "        return {'true_seg': true_seg_resized, 'pred_seg': pred_seg_resized}\n",
        "\n",
        "    def resize_3d(self, volume, target_shape):\n",
        "        \"\"\"\n",
        "        Resize a 3D PyTorch tensor to a target shape using trilinear interpolation.\n",
        "        :param volume: Input 3D PyTorch tensor\n",
        "        :param target_shape: Desired shape (D, H, W)\n",
        "        :return: Resized 3D PyTorch tensor\n",
        "        \"\"\"\n",
        "        # Ensure the tensor has at least 3 spatial dimensions\n",
        "        if len(volume.shape) == 3:  # Shape [D, H, W]\n",
        "            volume = volume.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions [1, 1, D, H, W]\n",
        "        elif len(volume.shape) == 4:  # Shape [C, D, H, W]\n",
        "            volume = volume.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Check the shape before interpolation to make sure it's valid\n",
        "        if volume.dim() != 5:\n",
        "            raise ValueError(f\"Expected a 5D tensor, but got {volume.dim()}D tensor with shape {volume.shape}\")\n",
        "\n",
        "        # Perform the interpolation (resizing)\n",
        "        resized_volume = torch.nn.functional.interpolate(volume, size=target_shape, mode='trilinear', align_corners=False)\n",
        "\n",
        "        # Remove the batch and channel dimensions\n",
        "        return resized_volume.squeeze(0).squeeze(0)  # Back to shape [D, H, W]\n",
        "\n",
        "\n",
        "# Load data in PyTorch\n",
        "true_segmentations_dir = '/content/drive/MyDrive/Mr-Alizadeh/Train-760/CT-segmentation'\n",
        "predicted_masks_dir = '/content/drive/MyDrive/Mr-Alizadeh/Reconnet_predicted_masks'\n",
        "\n",
        "# Create dataset (PyTorch)\n",
        "dataset = SegmentationDataset(true_seg_dir=true_segmentations_dir, pred_seg_dir=predicted_masks_dir, transform=ToTensorAndResize())\n",
        "\n",
        "# Initialize the results dictionary for TensorFlow metrics\n",
        "results_tf = {}\n",
        "\n",
        "# Iterate through the dataset and calculate metrics using TensorFlow\n",
        "for i in range(30,60):\n",
        "    sample = dataset[i]\n",
        "\n",
        "    # Convert PyTorch tensors to NumPy and then TensorFlow tensors\n",
        "    y_true_np = sample['true_seg'].numpy()\n",
        "    y_pred_np = sample['pred_seg'].numpy()\n",
        "\n",
        "    y_true_tf = tf.convert_to_tensor(y_true_np, dtype=tf.float32)\n",
        "    y_pred_tf = tf.convert_to_tensor(y_pred_np, dtype=tf.float32)\n",
        "\n",
        "    # Flatten the tensors for certain metrics\n",
        "    y_true_flat = tf.reshape(y_true_tf, [-1])\n",
        "    y_pred_flat = tf.reshape(y_pred_tf, [-1])\n",
        "\n",
        "    # Convert to binary format for metrics\n",
        "    y_true_bin = tf.cast(y_true_flat, dtype=tf.int32)\n",
        "    y_pred_bin = tf.cast(y_pred_flat, dtype=tf.int32)\n",
        "\n",
        "    # TensorFlow/Keras metrics\n",
        "    accuracy = BinaryAccuracy()\n",
        "    precision = Precision()\n",
        "    recall = Recall()\n",
        "    iou = MeanIoU(num_classes=2)\n",
        "\n",
        "    # Update and compute the metrics\n",
        "    accuracy.update_state(y_true_bin, y_pred_bin)\n",
        "    precision.update_state(y_true_bin, y_pred_bin)\n",
        "    recall.update_state(y_true_bin, y_pred_bin)\n",
        "    iou.update_state(y_true_bin, y_pred_bin)\n",
        "\n",
        "    # Calculate boundary F1 and Hausdorff Distance\n",
        "    def extract_boundaries_tf(mask):\n",
        "        eroded_mask = tf.nn.max_pool3d(mask[None, None, ...], ksize=3, strides=1, padding='SAME')[0, 0]\n",
        "        boundary = tf.cast(tf.not_equal(mask, eroded_mask), dtype=tf.float32)\n",
        "        return boundary\n",
        "\n",
        "    def boundary_f1_score_tf(y_true, y_pred, dilation_radius=1):\n",
        "        y_true_boundary = extract_boundaries_tf(y_true)\n",
        "        y_pred_boundary = extract_boundaries_tf(y_pred)\n",
        "        y_true_boundary_flat = tf.reshape(y_true_boundary, [-1])\n",
        "        y_pred_boundary_flat = tf.reshape(y_pred_boundary, [-1])\n",
        "        precision_value = tf.reduce_sum(y_true_boundary_flat * y_pred_boundary_flat) / (tf.reduce_sum(y_pred_boundary_flat) + 1e-6)\n",
        "        recall_value = tf.reduce_sum(y_true_boundary_flat * y_pred_boundary_flat) / (tf.reduce_sum(y_true_boundary_flat) + 1e-6)\n",
        "        bf_score = 2 * (precision_value * recall_value) / (precision_value + recall_value + 1e-6)\n",
        "        return bf_score.numpy()\n",
        "\n",
        "    def hausdorff_distance_tf(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate the Hausdorff Distance between two 3D binary masks.\n",
        "        :param y_true: Ground truth mask\n",
        "        :param y_pred: Predicted mask\n",
        "        :return: Hausdorff Distance\n",
        "        \"\"\"\n",
        "        # Get the indices of the true and predicted points\n",
        "        y_true_points = tf.where(tf.equal(y_true, 1))\n",
        "        y_pred_points = tf.where(tf.equal(y_pred, 1))\n",
        "\n",
        "        # Cast the points to float32 for distance calculation\n",
        "        y_true_points = tf.cast(y_true_points, dtype=tf.float32)\n",
        "        y_pred_points = tf.cast(y_pred_points, dtype=tf.float32)\n",
        "\n",
        "        # Handle cases where there are no points in either y_true or y_pred\n",
        "        if tf.shape(y_true_points)[0] == 0 or tf.shape(y_pred_points)[0] == 0:\n",
        "            return float('inf')  # Return infinity if there are no points to compare\n",
        "\n",
        "        # Calculate pairwise distances between points in y_true and y_pred\n",
        "        dists = tf.norm(tf.expand_dims(y_true_points, 1) - tf.expand_dims(y_pred_points, 0), axis=-1)\n",
        "\n",
        "        # Hausdorff distance is the maximum of the minimum distances from y_true to y_pred and vice versa\n",
        "        forward_hd = tf.reduce_max(tf.reduce_min(dists, axis=1))\n",
        "        backward_hd = tf.reduce_max(tf.reduce_min(dists, axis=0))\n",
        "\n",
        "        # Return the Hausdorff distance\n",
        "        return tf.reduce_max([forward_hd, backward_hd]).numpy()\n",
        "\n",
        "\n",
        "    # Calculate boundary F1 and Hausdorff Distance\n",
        "    bf_score_tf = boundary_f1_score_tf(y_true_tf, y_pred_tf)\n",
        "    hd_tf = hausdorff_distance_tf(y_true_tf, y_pred_tf)\n",
        "\n",
        "    # Store results\n",
        "    results_tf[f'Sample {i}'] = {\n",
        "        'Accuracy': accuracy.result().numpy(),\n",
        "        'Precision': precision.result().numpy(),\n",
        "        'Recall': recall.result().numpy(),\n",
        "        'IoU': iou.result().numpy(),\n",
        "        'BF Score': bf_score_tf,\n",
        "        'Hausdorff Distance': hd_tf\n",
        "    }\n",
        "\n",
        "# Convert results to a DataFrame for better readability\n",
        "results_tf_df = pd.DataFrame(results_tf).T\n",
        "results_tf_df.index.name = 'Sample'\n",
        "results_tf_df.reset_index(inplace=True)\n",
        "print(results_tf_df)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "# os.makedirs('./results', exist_ok=True)\n",
        "results_tf_df.to_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/binary_TF30-59.xlsx', index=False)\n",
        "# Torch 30-59"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogmPJ_z2fJz-"
      },
      "source": [
        "# TF 60-99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sE5XMxWHeHQ8",
        "outputId": "fde11e7f-4200-4959-c8eb-6184518d5ccb"
      },
      "outputs": [],
      "source": [
        "# TF 60-99\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from torch.utils.data import Dataset\n",
        "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy, MeanIoU\n",
        "\n",
        "np.set_printoptions(precision=25)\n",
        "pd.options.display.float_format = '{:.25f}'.format\n",
        "\n",
        "# SegmentationDataset Class (PyTorch)\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, true_seg_dir, pred_seg_dir, transform=None):\n",
        "        self.true_seg_dir = true_seg_dir\n",
        "        self.pred_seg_dir = pred_seg_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all files in directories\n",
        "        self.true_seg_files = sorted(os.listdir(true_seg_dir))\n",
        "        self.pred_seg_files = sorted(os.listdir(pred_seg_dir))\n",
        "\n",
        "        # Extract the common parts (e.g. #LIDC-IDRI#LIDC-IDRI-0001#VISIT 1#.nii.gz)\n",
        "        self.true_seg_base_names = [self.extract_base_name(file) for file in self.true_seg_files]\n",
        "        self.pred_seg_base_names = [self.extract_base_name(file) for file in self.pred_seg_files]\n",
        "\n",
        "        # Ensure that there are matching filenames between true and predicted\n",
        "        assert set(self.true_seg_base_names) == set(self.pred_seg_base_names), \\\n",
        "            \"True and predicted segmentation filenames do not match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.true_seg_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        base_name = self.true_seg_base_names[idx]\n",
        "        true_seg_filename = [file for file in self.true_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "        pred_seg_filename = [file for file in self.pred_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "\n",
        "        true_seg_path = os.path.join(self.true_seg_dir, true_seg_filename)\n",
        "        pred_seg_path = os.path.join(self.pred_seg_dir, pred_seg_filename)\n",
        "\n",
        "        true_seg_image = nib.load(true_seg_path).get_fdata()\n",
        "        pred_seg_image = nib.load(pred_seg_path).get_fdata()\n",
        "\n",
        "        # Ensure segmentation masks are binary (0 or 1)\n",
        "        true_seg_image = np.expand_dims(np.clip(true_seg_image, 0, 1), axis=0)\n",
        "        pred_seg_image = np.expand_dims(np.clip(pred_seg_image, 0, 1), axis=0)\n",
        "\n",
        "        sample = {'true_seg': true_seg_image, 'pred_seg': pred_seg_image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def extract_base_name(self, file_name):\n",
        "        base_name = file_name.split('#', 1)[1]\n",
        "        return base_name\n",
        "\n",
        "class ToTensorAndResize(object):\n",
        "    def __call__(self, sample):\n",
        "        true_seg = torch.from_numpy(sample['true_seg']).float()\n",
        "        pred_seg = torch.from_numpy(sample['pred_seg']).float()\n",
        "\n",
        "        # Reshape the tensors to target size of 128x128x128 (D, H, W)\n",
        "        true_seg_resized = self.resize_3d(true_seg, (128, 128, 128))\n",
        "        pred_seg_resized = self.resize_3d(pred_seg, (128, 128, 128))\n",
        "\n",
        "        return {'true_seg': true_seg_resized, 'pred_seg': pred_seg_resized}\n",
        "\n",
        "    def resize_3d(self, volume, target_shape):\n",
        "        \"\"\"\n",
        "        Resize a 3D PyTorch tensor to a target shape using trilinear interpolation.\n",
        "        :param volume: Input 3D PyTorch tensor\n",
        "        :param target_shape: Desired shape (D, H, W)\n",
        "        :return: Resized 3D PyTorch tensor\n",
        "        \"\"\"\n",
        "        # Ensure the tensor has at least 3 spatial dimensions\n",
        "        if len(volume.shape) == 3:  # Shape [D, H, W]\n",
        "            volume = volume.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions [1, 1, D, H, W]\n",
        "        elif len(volume.shape) == 4:  # Shape [C, D, H, W]\n",
        "            volume = volume.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Check the shape before interpolation to make sure it's valid\n",
        "        if volume.dim() != 5:\n",
        "            raise ValueError(f\"Expected a 5D tensor, but got {volume.dim()}D tensor with shape {volume.shape}\")\n",
        "\n",
        "        # Perform the interpolation (resizing)\n",
        "        resized_volume = torch.nn.functional.interpolate(volume, size=target_shape, mode='trilinear', align_corners=False)\n",
        "\n",
        "        # Remove the batch and channel dimensions\n",
        "        return resized_volume.squeeze(0).squeeze(0)  # Back to shape [D, H, W]\n",
        "\n",
        "\n",
        "# Load data in PyTorch\n",
        "true_segmentations_dir = '/content/drive/MyDrive/Mr-Alizadeh/Train-760/CT-segmentation'\n",
        "predicted_masks_dir = '/content/drive/MyDrive/Mr-Alizadeh/Reconnet_predicted_masks'\n",
        "\n",
        "# Create dataset (PyTorch)\n",
        "dataset = SegmentationDataset(true_seg_dir=true_segmentations_dir, pred_seg_dir=predicted_masks_dir, transform=ToTensorAndResize())\n",
        "\n",
        "# Initialize the results dictionary for TensorFlow metrics\n",
        "results_tf = {}\n",
        "\n",
        "# Iterate through the dataset and calculate metrics using TensorFlow\n",
        "for i in range(60,100):\n",
        "    sample = dataset[i]\n",
        "\n",
        "    # Convert PyTorch tensors to NumPy and then TensorFlow tensors\n",
        "    y_true_np = sample['true_seg'].numpy()\n",
        "    y_pred_np = sample['pred_seg'].numpy()\n",
        "\n",
        "    y_true_tf = tf.convert_to_tensor(y_true_np, dtype=tf.float32)\n",
        "    y_pred_tf = tf.convert_to_tensor(y_pred_np, dtype=tf.float32)\n",
        "\n",
        "    # Flatten the tensors for certain metrics\n",
        "    y_true_flat = tf.reshape(y_true_tf, [-1])\n",
        "    y_pred_flat = tf.reshape(y_pred_tf, [-1])\n",
        "\n",
        "    # Convert to binary format for metrics\n",
        "    y_true_bin = tf.cast(y_true_flat, dtype=tf.int32)\n",
        "    y_pred_bin = tf.cast(y_pred_flat, dtype=tf.int32)\n",
        "\n",
        "    # TensorFlow/Keras metrics\n",
        "    accuracy = BinaryAccuracy()\n",
        "    precision = Precision()\n",
        "    recall = Recall()\n",
        "    iou = MeanIoU(num_classes=2)\n",
        "\n",
        "    # Update and compute the metrics\n",
        "    accuracy.update_state(y_true_bin, y_pred_bin)\n",
        "    precision.update_state(y_true_bin, y_pred_bin)\n",
        "    recall.update_state(y_true_bin, y_pred_bin)\n",
        "    iou.update_state(y_true_bin, y_pred_bin)\n",
        "\n",
        "    # Calculate boundary F1 and Hausdorff Distance\n",
        "    def extract_boundaries_tf(mask):\n",
        "        eroded_mask = tf.nn.max_pool3d(mask[None, None, ...], ksize=3, strides=1, padding='SAME')[0, 0]\n",
        "        boundary = tf.cast(tf.not_equal(mask, eroded_mask), dtype=tf.float32)\n",
        "        return boundary\n",
        "\n",
        "    def boundary_f1_score_tf(y_true, y_pred, dilation_radius=1):\n",
        "        y_true_boundary = extract_boundaries_tf(y_true)\n",
        "        y_pred_boundary = extract_boundaries_tf(y_pred)\n",
        "        y_true_boundary_flat = tf.reshape(y_true_boundary, [-1])\n",
        "        y_pred_boundary_flat = tf.reshape(y_pred_boundary, [-1])\n",
        "        precision_value = tf.reduce_sum(y_true_boundary_flat * y_pred_boundary_flat) / (tf.reduce_sum(y_pred_boundary_flat) + 1e-6)\n",
        "        recall_value = tf.reduce_sum(y_true_boundary_flat * y_pred_boundary_flat) / (tf.reduce_sum(y_true_boundary_flat) + 1e-6)\n",
        "        bf_score = 2 * (precision_value * recall_value) / (precision_value + recall_value + 1e-6)\n",
        "        return bf_score.numpy()\n",
        "\n",
        "    def hausdorff_distance_tf(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate the Hausdorff Distance between two 3D binary masks.\n",
        "        :param y_true: Ground truth mask\n",
        "        :param y_pred: Predicted mask\n",
        "        :return: Hausdorff Distance\n",
        "        \"\"\"\n",
        "        # Get the indices of the true and predicted points\n",
        "        y_true_points = tf.where(tf.equal(y_true, 1))\n",
        "        y_pred_points = tf.where(tf.equal(y_pred, 1))\n",
        "\n",
        "        # Cast the points to float32 for distance calculation\n",
        "        y_true_points = tf.cast(y_true_points, dtype=tf.float32)\n",
        "        y_pred_points = tf.cast(y_pred_points, dtype=tf.float32)\n",
        "\n",
        "        # Handle cases where there are no points in either y_true or y_pred\n",
        "        if tf.shape(y_true_points)[0] == 0 or tf.shape(y_pred_points)[0] == 0:\n",
        "            return float('inf')  # Return infinity if there are no points to compare\n",
        "\n",
        "        # Calculate pairwise distances between points in y_true and y_pred\n",
        "        dists = tf.norm(tf.expand_dims(y_true_points, 1) - tf.expand_dims(y_pred_points, 0), axis=-1)\n",
        "\n",
        "        # Hausdorff distance is the maximum of the minimum distances from y_true to y_pred and vice versa\n",
        "        forward_hd = tf.reduce_max(tf.reduce_min(dists, axis=1))\n",
        "        backward_hd = tf.reduce_max(tf.reduce_min(dists, axis=0))\n",
        "\n",
        "        # Return the Hausdorff distance\n",
        "        return tf.reduce_max([forward_hd, backward_hd]).numpy()\n",
        "\n",
        "\n",
        "    # Calculate boundary F1 and Hausdorff Distance\n",
        "    bf_score_tf = boundary_f1_score_tf(y_true_tf, y_pred_tf)\n",
        "    hd_tf = hausdorff_distance_tf(y_true_tf, y_pred_tf)\n",
        "\n",
        "    # Store results\n",
        "    results_tf[f'Sample {i}'] = {\n",
        "        'Accuracy': accuracy.result().numpy(),\n",
        "        'Precision': precision.result().numpy(),\n",
        "        'Recall': recall.result().numpy(),\n",
        "        'IoU': iou.result().numpy(),\n",
        "        'BF Score': bf_score_tf,\n",
        "        'Hausdorff Distance': hd_tf\n",
        "    }\n",
        "\n",
        "# Convert results to a DataFrame for better readability\n",
        "results_tf_df = pd.DataFrame(results_tf).T\n",
        "results_tf_df.index.name = 'Sample'\n",
        "results_tf_df.reset_index(inplace=True)\n",
        "print(results_tf_df)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "# os.makedirs('./results', exist_ok=True)\n",
        "results_tf_df.to_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/binary_TF60-99.xlsx', index=False)\n",
        "# Torch 60-99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfsDSMUvfozz"
      },
      "source": [
        "# Torch 760 pic - bach 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PVOCEaJ3aUZ2",
        "outputId": "6cdd79a8-8514-4f78-d9fd-721062e65138"
      },
      "outputs": [],
      "source": [
        "# Torch 760 pic - bach 10\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchmetrics\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nibabel as nib\n",
        "import os\n",
        "from decimal import getcontext, Decimal\n",
        "from torchmetrics.functional import dice\n",
        "\n",
        "np.set_printoptions(precision=25)\n",
        "getcontext().prec = 25\n",
        "pd.options.display.float_format = '{:.25f}'.format\n",
        "\n",
        "# Set precision for numpy\n",
        "np.set_printoptions(precision=25)\n",
        "\n",
        "# Set precision for pandas\n",
        "pd.set_option('display.float_format', '{:.25f}'.format)\n",
        "\n",
        "# SegmentationDataset Class\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, true_seg_dir, pred_seg_dir, transform=None):\n",
        "        self.true_seg_dir = true_seg_dir\n",
        "        self.pred_seg_dir = pred_seg_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get all files in directories\n",
        "        self.true_seg_files = sorted(os.listdir(true_seg_dir))\n",
        "        self.pred_seg_files = sorted(os.listdir(pred_seg_dir))\n",
        "\n",
        "        # Extract the common parts (e.g. #LIDC-IDRI#LIDC-IDRI-0001#VISIT 1#.nii.gz)\n",
        "        self.true_seg_base_names = [self.extract_base_name(file) for file in self.true_seg_files]\n",
        "        self.pred_seg_base_names = [self.extract_base_name(file) for file in self.pred_seg_files]\n",
        "\n",
        "        # Ensure that there are matching filenames between true and predicted\n",
        "        assert set(self.true_seg_base_names) == set(self.pred_seg_base_names), \\\n",
        "            \"True and predicted segmentation filenames do not match.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.true_seg_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Find the matching true and predicted segmentation files\n",
        "        base_name = self.true_seg_base_names[idx]\n",
        "        true_seg_filename = [file for file in self.true_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "        pred_seg_filename = [file for file in self.pred_seg_files if self.extract_base_name(file) == base_name][0]\n",
        "\n",
        "        true_seg_path = os.path.join(self.true_seg_dir, true_seg_filename)\n",
        "        pred_seg_path = os.path.join(self.pred_seg_dir, pred_seg_filename)\n",
        "\n",
        "        true_seg_image = nib.load(true_seg_path).get_fdata()\n",
        "        pred_seg_image = nib.load(pred_seg_path).get_fdata()\n",
        "\n",
        "        # Ensure segmentation masks are binary (if needed)\n",
        "        true_seg_image = np.expand_dims(np.clip(true_seg_image, 0, 1), axis=0)\n",
        "        pred_seg_image = np.expand_dims(np.clip(pred_seg_image, 0, 1), axis=0)\n",
        "\n",
        "        sample = {'true_seg': true_seg_image, 'pred_seg': pred_seg_image}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def extract_base_name(self, file_name):\n",
        "        base_name = file_name.split('#', 1)[1]\n",
        "        return base_name\n",
        "\n",
        "\n",
        "# ToTensor Class\n",
        "class ToTensor(object):\n",
        "    def __call__(self, sample):\n",
        "        true_seg = torch.from_numpy(sample['true_seg'])\n",
        "        pred_seg = torch.from_numpy(sample['pred_seg'])\n",
        "        true_seg, pred_seg = true_seg.type(torch.float32), pred_seg.type(torch.float32)\n",
        "\n",
        "        x, y, z = 256, 256, 128  # Example target shape (D, H, W)\n",
        "        true_seg_resized = F.interpolate(true_seg.unsqueeze(0), size=(x, y, z), mode='trilinear', align_corners=False).squeeze(0)\n",
        "        pred_seg_resized = F.interpolate(pred_seg.unsqueeze(0), size=(x, y, z), mode='trilinear', align_corners=False).squeeze(0)\n",
        "\n",
        "        return {'true_seg': true_seg_resized, 'pred_seg': pred_seg_resized}\n",
        "\n",
        "\n",
        "# Example usage\n",
        "true_segmentations_dir = '/content/drive/MyDrive/Mr-Alizadeh/Train-760/CT-segmentation'\n",
        "predicted_masks_dir = '/content/drive/MyDrive/Mr-Alizadeh/Reconnet_predicted_masks'\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = SegmentationDataset(true_seg_dir=true_segmentations_dir, pred_seg_dir=predicted_masks_dir, transform=ToTensor())\n",
        "\n",
        "# Load data in batches of 10\n",
        "dataloader = DataLoader(dataset, batch_size=10, shuffle=False)\n",
        "\n",
        "# Initialize the results dictionary\n",
        "results2 = {}\n",
        "\n",
        "# Iterate through the dataloader and calculate metrics\n",
        "for i, sample_batch in enumerate(dataloader):\n",
        "    for j in range(len(sample_batch['true_seg'])):\n",
        "        y_true = sample_batch['true_seg'][j]\n",
        "        y_pred = sample_batch['pred_seg'][j]\n",
        "\n",
        "        # Ensure unique samples are being loaded\n",
        "        print(f\"Processing Batch {i}, Sample {j}: y_true.shape = {y_true.shape}, y_pred.shape = {y_pred.shape}\")\n",
        "\n",
        "        # Clone and detach the tensors to avoid any computational graph tracking\n",
        "        y_true = y_true.clone().detach().type(torch.float32)\n",
        "        y_pred = y_pred.clone().detach().type(torch.float32)\n",
        "\n",
        "        # Flatten the images for certain metrics\n",
        "        y_true_flat = y_true.flatten()\n",
        "        y_pred_flat = y_pred.flatten()\n",
        "\n",
        "        # PyTorch metrics\n",
        "        y_true_torch = y_true_flat.type(torch.int)\n",
        "        y_pred_torch = y_pred_flat.type(torch.int)\n",
        "        # Boundary F1 Score and Hausdorff Distance functions\n",
        "        def extract_boundaries_torch(mask):\n",
        "            \"\"\"Extract boundary pixels from a binary segmentation mask using PyTorch.\"\"\"\n",
        "            if len(mask.shape) == 3:\n",
        "                mask = mask.unsqueeze(0).unsqueeze(0)\n",
        "            if mask.dim() == 5:\n",
        "                eroded_mask = F.max_pool3d(mask, kernel_size=3, stride=1, padding=1).squeeze()\n",
        "            else:\n",
        "                eroded_mask = F.max_pool2d(mask, kernel_size=3, stride=1, padding=1).squeeze()\n",
        "            boundary = (mask.squeeze() != eroded_mask).float()\n",
        "            return boundary\n",
        "\n",
        "\n",
        "        def custom_dilation_torch(mask, kernel_size=3):\n",
        "            \"\"\"Dilate the boundary pixels for 3D data.\"\"\"\n",
        "            kernel = torch.ones((1, 1, kernel_size, kernel_size, kernel_size), dtype=torch.float32)\n",
        "            mask = mask.unsqueeze(0).unsqueeze(0)\n",
        "            dilated = F.conv3d(mask, kernel, padding=kernel_size // 2).squeeze()\n",
        "            dilated = (dilated > 0).float()\n",
        "            return dilated\n",
        "\n",
        "\n",
        "        def boundary_f1_score_torch(y_true, y_pred, dilation_radius=1):\n",
        "            y_true_boundary = extract_boundaries_torch(y_true)\n",
        "            y_pred_boundary = extract_boundaries_torch(y_pred)\n",
        "            y_true_boundary_dilated = custom_dilation_torch(y_true_boundary, kernel_size=2 * dilation_radius + 1)\n",
        "            y_pred_boundary_dilated = custom_dilation_torch(y_pred_boundary, kernel_size=2 * dilation_radius + 1)\n",
        "            y_true_boundary_flat = y_true_boundary_dilated.flatten()\n",
        "            y_pred_boundary_flat = y_pred_boundary_dilated.flatten()\n",
        "            precision_metric = torchmetrics.Precision(task=\"binary\")\n",
        "            recall_metric = torchmetrics.Recall(task=\"binary\")\n",
        "            precision_metric.update(y_pred_boundary_flat, y_true_boundary_flat)\n",
        "            recall_metric.update(y_pred_boundary_flat, y_true_boundary_flat)\n",
        "            precision = precision_metric.compute().item()\n",
        "            recall = recall_metric.compute().item()\n",
        "            if precision + recall > 0:\n",
        "                bf1_score = 2 * (precision * recall) / (precision + recall)\n",
        "            else:\n",
        "                bf1_score = 0.0\n",
        "            return bf1_score\n",
        "\n",
        "\n",
        "        def hausdorff_distance_torch(y_true, y_pred):\n",
        "            y_true_boundary = extract_boundaries_torch(y_true)\n",
        "            y_pred_boundary = extract_boundaries_torch(y_pred)\n",
        "            y_true_coords = torch.nonzero(y_true_boundary)\n",
        "            y_pred_coords = torch.nonzero(y_pred_boundary)\n",
        "            if len(y_true_coords) == 0 or len(y_pred_coords) == 0:\n",
        "                return torch.tensor(float('inf'))\n",
        "            diff = torch.cdist(y_true_coords.float(), y_pred_coords.float(), p=2)\n",
        "            hd = torch.max(torch.min(diff, dim=0)[0].max(), torch.min(diff, dim=1)[0].max())\n",
        "            return hd.item()\n",
        "        accuracy_metric = torchmetrics.Accuracy(task=\"binary\")\n",
        "        precision_metric = torchmetrics.Precision(task=\"binary\")\n",
        "        recall_metric = torchmetrics.Recall(task=\"binary\")\n",
        "        f1_metric = torchmetrics.F1Score(task=\"binary\")\n",
        "        iou_metric = torchmetrics.JaccardIndex(task=\"binary\")\n",
        "\n",
        "        accuracy_metric.update(y_pred_torch, y_true_torch)\n",
        "        precision_metric.update(y_pred_torch, y_true_torch)\n",
        "        recall_metric.update(y_pred_torch, y_true_torch)\n",
        "        f1_metric.update(y_pred_torch, y_true_torch)\n",
        "        iou_metric.update(y_pred_torch, y_true_torch)\n",
        "\n",
        "        pt_accuracy = accuracy_metric.compute().item()\n",
        "        pt_precision = precision_metric.compute().item()\n",
        "        pt_recall = recall_metric.compute().item()\n",
        "        pt_f1 = f1_metric.compute().item()\n",
        "        pt_iou = iou_metric.compute().item()\n",
        "\n",
        "        bf_score_torch = boundary_f1_score_torch(y_true, y_pred)\n",
        "        hd_torch = hausdorff_distance_torch(y_true, y_pred)\n",
        "\n",
        "        # Save all results for each sample in the batch\n",
        "        results2[f'Batch {i}, Sample {j}'] = {\n",
        "            'Accuracy': pt_accuracy,\n",
        "            'Precision': pt_precision,\n",
        "            'Recall': pt_recall,\n",
        "            'F1 Score': pt_f1,\n",
        "            'IoU': pt_iou,\n",
        "            'Dice Coefficient': dice(y_pred_torch, y_true_torch).item(),\n",
        "            'BF Score': bf_score_torch,\n",
        "            'Hausdorff Distance': hd_torch\n",
        "        }\n",
        "        results2_df = pd.DataFrame(results2).T\n",
        "        results2_df.index.name = 'Sample'\n",
        "        results2_df.reset_index(inplace=True)\n",
        "        print(results2_df)\n",
        "        results2_df.to_excel(f'/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/binary_TORCH_Bach_metrics{i}.xlsx', index=False)\n",
        "\n",
        "\n",
        "# Convert results to a DataFrame for better readability\n",
        "results2_df = pd.DataFrame(results2).T\n",
        "results2_df.index.name = 'Sample'\n",
        "results2_df.reset_index(inplace=True)\n",
        "print(results2_df)\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "results2_df.to_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Segmentation/binary_TORCH_Bach_metrics.xlsx', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MRQlwBDXgC_8",
        "PsZoiibYf4dr",
        "uE1zIhkvfeHc",
        "7-NyZUQ8fQh_",
        "ogmPJ_z2fJz-",
        "cfsDSMUvfozz"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
