{"cells":[{"cell_type":"markdown","metadata":{"id":"b-0E3xbWB8u8"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":87199,"status":"ok","timestamp":1728395342743,"user":{"displayName":"Morteza Alizadeh","userId":"14487711583328697886"},"user_tz":-210},"id":"Toxl4idWfHU1","outputId":"0f3c630e-bf26-44d3-9aca-a2c7c029b2b8"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2558,"status":"ok","timestamp":1728395351121,"user":{"displayName":"Morteza Alizadeh","userId":"14487711583328697886"},"user_tz":-210},"id":"A1YnXLBLfTTk","outputId":"0d118bb0-30f1-48b9-d54e-56dc66a242ff"},"outputs":[],"source":["%cd /content/drive/MyDrive/Loss Function Evaluation/Data/Correlation and statistical test/\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":91259,"status":"ok","timestamp":1728395448620,"user":{"displayName":"Morteza Alizadeh","userId":"14487711583328697886"},"user_tz":-210},"id":"k4zr0YIcdjdk","outputId":"46de21d0-def4-4aa0-ed38-dbe8e4a5e95c"},"outputs":[],"source":["!pip install pingouin\n","!pip install dcor\n","!pip install vaex\n","!pip install xarray\n","!pip install pyspark\n","!pip install geopandas\n","!pip install causalimpact\n","!pip install tensorflow\n","!pip install tensorflow-probability\n","!pip install torch torchvision torchaudio\n","!pip install keras\n","!pip install dask\n","!pip install seaborn\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"60mhu2mYdZY0"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","import json\n","import scipy.stats as sc\n","import statsmodels.api as sm\n","import pingouin as pg\n","import dcor\n","import vaex\n","import xarray as xr\n","import geopandas as gpd\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","import torch\n","import dask.array as da\n","import seaborn as sns\n","from scipy.stats import rankdata\n","from sklearn.metrics import pairwise_distances\n","from scipy.stats import entropy\n","from sklearn.metrics import mutual_info_score\n","from scipy.spatial.distance import pdist, squareform\n","from sklearn.preprocessing import StandardScaler\n","from scipy.stats import rankdata\n","from keras import backend as K\n","from causalimpact import CausalImpact\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import corr\n","from scipy.stats import pearsonr, spearmanr, kendalltau, pointbiserialr\n","from sklearn.datasets import make_regression\n","from sklearn.feature_selection import mutual_info_regression"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13257,"status":"ok","timestamp":1728395703052,"user":{"displayName":"Morteza Alizadeh","userId":"14487711583328697886"},"user_tz":-210},"id":"p97meQqufVUc","outputId":"e12527d7-4538-48ee-85a0-0f08df0a3abb"},"outputs":[],"source":["# Data 1\n","\n","mri_features = pd.read_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Classification/Multiclass/MRI_Radiomics Features.xlsx')\n","us_mri_features = pd.read_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Classification/Multiclass/US_MRI_Radiomics Features.xlsx')\n","ucla_data = pd.read_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Classification/Multiclass/UCLA.xlsx')\n","\n","print(f'MRI_Features: {mri_features.shape}')\n","print(f'us_mri_features: {us_mri_features.shape}')\n","print(f'ucla_data: {ucla_data.shape}')\n","\n","# Update lesion names in ucla_data\n","ucla_data['lesion_name'] = ucla_data['lesion_name'].str.replace('_US_Prostate', '')\n","\n","# Merge data based on lesion_name and PatientID\n","merged_data = ucla_data.merge(mri_features, left_on='lesion_name', right_on='PatientID')\n","\n","\n","# Remove a specific class (for example, class 1) from multi-class data\n","class_to_remove = 1\n","filtered_data = merged_data[merged_data['UCLA Score (Similar to PIRADS v2)_Multi-class'] != 0]\n","filtered_data = filtered_data[filtered_data['UCLA Score (Similar to PIRADS v2)_Multi-class'] != 1]\n","filtered_data = filtered_data[filtered_data['UCLA Score (Similar to PIRADS v2)_Multi-class'] != 2]\n","\n","\n","# Drop name columns and separate features and target variables\n","features_filtered = filtered_data.drop(columns=['lesion_name', 'PatientID', 'Labels_Binary', 'Label'])\n","# y_true_multi_filtered = filtered_data['UCLA Score (Similar to PIRADS v2)_Multi-class']\n","y_true_bi_filtered = filtered_data['Labels_Binary']\n","\n","# Split data into training and test sets\n","# X_train, X_test, y_train, y_test = train_test_split(features_filtered, y_true_multi_filtered, test_size=0.2, random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(features_filtered, y_true_bi_filtered, test_size=0.2, random_state=42)\n","y_true_1 = y_test\n","y_true_1 = np.array(y_true_1)\n","\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjkOBiLvfbVM"},"outputs":[],"source":["np.random.seed(42)\n","x1 = np.random.rand(200)\n","y1 = np.random.rand(200)\n","df = pd.DataFrame({'x': x1, 'y': y1})\n","\n","x2 = features_filtered['morph_area_mesh']\n","y2 = features_filtered['ngl_dcnu_3D']\n","x2 = x2.to_numpy()\n","y2 = y2.to_numpy()\n","\n","x3 = features_filtered['ngl_hdhge_3D']\n","y3 = features_filtered['ngl_glnu_3D']\n","x3 = x3.to_numpy()\n","y3 = y3.to_numpy()\n","\n","x4 = features_filtered['morph_av']\n","y4 = features_filtered['ngl_lde_3D']\n","x4 = x4.to_numpy()\n","y4 = y4.to_numpy()\n","\n","\n","\n","df1 = pd.DataFrame({'x': x1, 'y': y1})\n","df2 = pd.DataFrame({'x': x2, 'y': y2})\n","df3 = pd.DataFrame({'x': x3, 'y': y3})\n","df4 = pd.DataFrame({'x': x4, 'y': y4})\n","\n","\n","with pd.ExcelWriter('/content/drive/MyDrive/Loss Function Evaluation/Data/Correlation and statistical test/Corr_output.xlsx') as writer:\n","    df1.to_excel(writer, sheet_name='random', index=False)\n","    df2.to_excel(writer, sheet_name='area_mesh-ngl_dcnu', index=False)\n","    df3.to_excel(writer, sheet_name='ngl_hdhge-ngl_glnu', index=False)\n","    df4.to_excel(writer, sheet_name='morph_av-ngl_lde', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jB4yTcNccSbS"},"outputs":[],"source":["data_corr = pd.read_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Correlation and statistical test/Corr_output.xlsx', sheet_name=\"morph_av-ngl_lde\")\n","x2 = data_corr['x']\n","y2 = data_corr['y']\n","x2 = np.array(x2)\n","y2 = np.array(y2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nj_jaLKxfcq6"},"outputs":[],"source":["x = x2\n","y = y2\n","np.random.seed(42)\n","binary_array = np.random.randint(2, size = x.shape)"]},{"cell_type":"markdown","metadata":{"id":"Bbnj7pwPDOd8"},"source":["Scipy Library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1fBvQbuDPLX"},"outputs":[],"source":["def scipy_corrs (x, y, binary_array):\n","  return {\n","      'Pearson': sc.pearsonr (x, y)[0],\n","      'Spearman': sc.spearmanr (x, y)[0],\n","      'Kendallâ€™s Tau': sc.kendalltau (x, y)[0],\n","      'Point-Biserial': sc.pointbiserialr (binary_array, y)[0]\n","  }\n"]},{"cell_type":"markdown","metadata":{"id":"7cMbdKufDe9p"},"source":["Scikit-Learn Library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4F0OOMeDYAR"},"outputs":[],"source":["def spearman_correlation_sklearn(x, y):\n","    # Convert to numpy arrays\n","    x = np.array(x)\n","    y = np.array(y)\n","\n","    # Rank the data\n","    x_rank = rankdata(x)\n","    y_rank = rankdata(y)\n","\n","    # Compute Pearson correlation coefficient on the ranks\n","    spearman_corr = np.corrcoef(x_rank, y_rank)[0, 1]\n","\n","    return spearman_corr\n","\n","def pearson_correlation_sklearn(x, y):\n","    \"\"\"\n","    Compute Pearson's correlation coefficient using sklearn for preprocessing.\n","\n","    Parameters:\n","    x (list or np.array): Continuous variable x.\n","    y (list or np.array): Continuous variable y.\n","\n","    Returns:\n","    float: Pearson's correlation coefficient.\n","    \"\"\"\n","    # Convert to numpy arrays if they are not already\n","    x = np.array(x)\n","    y = np.array(y)\n","\n","    # Standardize the data\n","    scaler = StandardScaler()\n","    x_scaled = scaler.fit_transform(x.reshape(-1, 1)).flatten()\n","    y_scaled = scaler.fit_transform(y.reshape(-1, 1)).flatten()\n","\n","    # Compute Pearson correlation coefficient\n","    pearson_corr = np.corrcoef(x_scaled, y_scaled)[0, 1]\n","\n","    return pearson_corr\n","\n","\n","\n","def sklearn_corrs (x, y):\n","  return {\n","      'Pearson': pearson_correlation_sklearn(x, y),\n","      'Spearman': spearman_correlation_sklearn(x, y) ,\n","      'Mutual Information': mutual_info_score(x, y)\n","  }\n"]},{"cell_type":"markdown","metadata":{"id":"NJYx-rQLFdjA"},"source":["Pandas Library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGIOsG57DpFJ"},"outputs":[],"source":["def distance_correlation(X, Y):\n","    X = np.asarray(X)\n","    Y = np.asarray(Y)\n","\n","    def distance_matrix(V):\n","        return np.abs(V[:, None] - V[None, :])\n","\n","    def centered_distance_matrix(D):\n","        n = D.shape[0]\n","        mean_i = D.mean(axis=1)\n","        mean_j = D.mean(axis=0)\n","        mean_ij = D.mean()\n","        return D - mean_i[:, None] - mean_j[None, :] + mean_ij\n","\n","    A = distance_matrix(X)\n","    B = distance_matrix(Y)\n","\n","    A_centered = centered_distance_matrix(A)\n","    B_centered = centered_distance_matrix(B)\n","\n","    n = X.shape[0]\n","    dCov_AB = np.sum(A_centered * B_centered) / (n ** 2)\n","\n","    dVar_A = np.sum(A_centered ** 2) / (n ** 2)\n","    dVar_B = np.sum(B_centered ** 2) / (n ** 2)\n","\n","    if dVar_A > 0 and dVar_B > 0:\n","        dCor = np.sqrt(dCov_AB) / np.sqrt(np.sqrt(dVar_A) * np.sqrt(dVar_B))\n","    else:\n","        dCor = 0.0\n","\n","    return dCor\n","\n","\n","\n","\n","def pandas_corrs (x, y, binary_array):\n","    df = pd.DataFrame({'x': x, 'y': y})\n","\n","    pxy =  pd.crosstab(x, y)/  pd.crosstab(x, y).sum().sum()\n","    px = pxy.sum(axis = 1)\n","    py = pxy.sum(axis = 0)\n","\n","    mi = 0\n","    for i in px.index:\n","        for j in py.index:\n","            pxy_ij = pxy.loc[i, j] if (i in pxy.index and j in pxy.columns) else 0\n","            if pxy_ij > 0:\n","                mi += pxy_ij * np.log(pxy_ij / (px[i] * py[j]))\n","\n","    return {\n","      'Pearson': df['x'].corr(df['y'], method ='pearson'),\n","      'Spearman': df['x'].corr(df['y'], method ='spearman'),\n","      'Kendallâ€™s Tau': df['x'].corr(df['y'], method ='kendall'),\n","      'Point-Biserial': (y[binary_array == 1].mean()- y[binary_array == 0].mean()) / y.std() * np.sqrt(binary_array.sum()   * (len(binary_array) - binary_array.sum()) / len(binary_array)**2),\n","      'Mutual Information': mi,\n","      'Distance Correlation': distance_correlation(x, y)\n","    }\n"]},{"cell_type":"markdown","metadata":{"id":"r2Wre7lEF4JT"},"source":["Numpy Library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"90IDYKENF3dM"},"outputs":[],"source":["def numpy_corrs (x, y) :\n","    x_rank = rankdata(x)\n","    y_rank = rankdata(y)\n","    tau, p = kendalltau(x_rank, y_rank)\n","    # calculating MI\n","    def mutual_information(x, y):\n","        # Ensure that x and y are numpy arrays\n","        x = np.asarray(x)\n","        y = np.asarray(y)\n","\n","        # Find unique values and their frequencies (probabilities)\n","        p_x, x_counts = np.unique(x, return_counts=True)\n","        p_x = x_counts / len(x)  # Marginal probability distribution for x\n","\n","        p_y, y_counts = np.unique(y, return_counts=True)\n","        p_y = y_counts / len(y)  # Marginal probability distribution for y\n","\n","        # Joint probability distribution\n","        joint_xy, _, _ = np.histogram2d(x, y, bins=[len(p_x), len(p_y)])\n","        joint_xy /= len(x)  # Normalize to get joint probabilities\n","\n","        # Calculate mutual information\n","        mi = 0.0\n","        for i in range(len(p_x)):\n","            for j in range(len(p_y)):\n","                if joint_xy[i, j] > 0:  # Avoid log(0)\n","                    mi += joint_xy[i, j] * np.log(joint_xy[i, j] / (p_x[i] * p_y[j]))\n","\n","        return mi\n","\n","    return {\n","        'Pearson': np.corrcoef(x, y)[0, 1],\n","        #custom implementation\n","        'Spearman': np.corrcoef(x_rank, y_rank)[0, 1],\n","        'Kendallâ€™s Tau': tau,\n","        'Mutual Information':mutual_information(x, y),\n","        #'Distance Correlation': distance_correlation(x, y)\n","    }"]},{"cell_type":"markdown","metadata":{"id":"wd7UP2GMGD-N"},"source":["Statsmodels Library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPCiQ5GYGBOl"},"outputs":[],"source":[" def kendall_tau_st(X, Y):\n","    X_rank = pd.Series(X).rank()\n","    Y_rank = pd.Series(Y).rank()\n","    n = len(X)\n","    concordant = 0\n","    discordant = 0\n","\n","    for i in range(n):\n","        for j in range(i + 1, n):\n","            if (X_rank[i] < X_rank[j] and Y_rank[i] < Y_rank[j]) or (X_rank[i] > X_rank[j] and Y_rank[i] > Y_rank[j]):\n","                concordant += 1\n","            elif (X_rank[i] < X_rank[j] and Y_rank[i] > Y_rank[j]) or (X_rank[i] > X_rank[j] and Y_rank[i] < Y_rank[j]):\n","                discordant += 1\n","\n","    tau = (concordant - discordant) / np.sqrt((concordant + discordant) * (n * (n - 1) / 2 - discordant))\n","    return tau\n","\n","\n","\n","def mutual_information_st(X, Y):\n","    X = pd.Series(X)\n","    Y = pd.Series(Y)\n","    # Compute joint frequency table\n","    joint_freq = pd.crosstab(X, Y, normalize = True)\n","\n","    # Compute marginal frequency tables\n","    p_x = joint_freq.sum(axis = 1)\n","    p_y = joint_freq.sum(axis = 0)\n","\n","    # Compute mutual information\n","    mi = 0.0\n","    for x in joint_freq.index:\n","        for y in joint_freq.columns:\n","            p_xy = joint_freq.loc[x, y]\n","            if p_xy > 0:  # Avoid division by zero\n","                mi += p_xy * np.log(p_xy / (p_x[x] * p_y[y]))\n","\n","    return mi\n","\n","def statsmodels_corrs (x, y, binary_array):\n","    df = pd.DataFrame({'x': x, 'y': y})\n","    corr_matrix = sm.tools.tools.add_constant(df.corr())\n","\n","\n","    return {\n","      'Pearson': corr_matrix.loc['x', 'y'],\n","      'Mutual Information': mutual_information_st(x, y),\n","\n","    }"]},{"cell_type":"markdown","metadata":{"id":"cpPExfZLGSkt"},"source":["Pingouin Library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6KHeDUvGGzO"},"outputs":[],"source":["def pingouin_corrs(x, y):\n","    z = np.random.rand(len(x))\n","    return {\n","        'Pearson': pg.corr(x = x, y = y, method ='pearson')['r'].values[0],\n","        'Spearman': pg.corr(x = x, y = y, method ='spearman')['r'].values[0],\n","        'Kendallâ€™s Tau': pg.corr( x = x, y = y, method ='kendall')['r'].values[0],\n","        'Bicor': pg.corr(x = x, y = y, method ='bicor')['r'].values[0],\n","        'Percbend': pg.corr(x = x, y = y, method ='percbend')['r'].values[0],\n","        'Shepherd': pg.corr(x = x, y = y, method ='shepherd')['r'].values[0],\n","        'partial_corr': pg.partial_corr(data = pd.DataFrame({'x': x, 'y': y, 'z': z}), x = 'x', y = 'y', covar = 'z')['r'].values[0],\n","    }\n"]},{"cell_type":"markdown","metadata":{"id":"ivUfnC_OGief"},"source":["Dcor Library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5HY-BNL6GbMK"},"outputs":[],"source":["def spearman_correlation_dcor(x, y):\n","    \"\"\"\n","    Compute Spearman's rank correlation coefficient using dcor for ranking.\n","\n","    Parameters:\n","    x (list or np.array or pd.Series): Continuous or ordinal variable x.\n","    y (list or np.array or pd.Series): Continuous or ordinal variable y.\n","\n","    Returns:\n","    float: Spearman's rank correlation coefficient.\n","    \"\"\"\n","    # Convert to pandas Series if they are numpy arrays or lists\n","    x = pd.Series(x)\n","    y = pd.Series(y)\n","\n","    x_rank = x.rank()\n","    y_rank = y.rank()\n","    dcor_corr = dcor.distance_correlation(x_rank, y_rank)\n","    spearman_corr = np.corrcoef(x_rank, y_rank)[0, 1]\n","\n","    return spearman_corr\n","\n","\n","def dcor_corrs(x, y):\n","  return {\n","      'Spearman': spearman_correlation_dcor(x, y),\n","      'Distance Correlation' : dcor.distance_correlation(x, y),\n","    }"]},{"cell_type":"markdown","metadata":{"id":"A1It3iYsH8y_"},"source":["Vaex Library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MV8E7dspG3GR"},"outputs":[],"source":["def vaex_corrs(x, y):\n","  dff = vaex.from_arrays(x = x, y = y)\n","  return {\n","      'Pearson': dff.correlation(dff.x, dff.y)\n","  }"]},{"cell_type":"markdown","metadata":{"id":"s9kNT1eFJ6dC"},"source":["XArray Library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffOVlP7jINAO"},"outputs":[],"source":["def xarray_corrs(x, y):\n","  #x_da = xr.DataArray(x)\n","  #y_da = xr.DataArray(y)\n","  x_da = xr.DataArray(x, dims=\"index\")\n","  y_da = xr.DataArray(y, dims=\"index\")\n","\n","  # Rank the data\n","  x_ranked = xr.apply_ufunc(rankdata, x_da)\n","  y_ranked = xr.apply_ufunc(rankdata, y_da)\n","\n","  # Calculate Pearson correlation on ranked data\n","  spearman_corr = xr.corr(x_ranked, y_ranked, dim=\"index\")\n","\n","\n","\n","  return {\n","      'Pearson': xr.corr(x_da, y_da).values,\n","      'Spearman': xr.corr(x_ranked, y_ranked, dim=\"index\").values,\n","\n","  }\n"]},{"cell_type":"markdown","metadata":{"id":"kfdJEg6OKBeY"},"source":["Spark Library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JD2gtCZZI25O"},"outputs":[],"source":["def spark_corrs(x, y):\n","  spark = SparkSession.builder.appName(\"PearsonCorrelation\").getOrCreate()\n","  data = [(float(x[i]), float(y[i])) for i in range(len(x))]\n","  df = spark.createDataFrame(data, [\"x\", \"y\"])\n","  return {\n","      'Pearson': df.select(corr(\"x\", \"y\")).collect()[0][0],\n","  }\n"]},{"cell_type":"markdown","metadata":{"id":"o9FQMyaZKfxz"},"source":["Geopandas Library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SucMlV6WJoti"},"outputs":[],"source":["def geopandas_corrs(x, y):\n","  df = pd.DataFrame({'x': x, 'y': y})\n","  gdf = gpd.GeoDataFrame(df)\n","  return {\n","      'Pearson': gdf['x'].corr(gdf['y'], method ='pearson'),\n","  }\n"]},{"cell_type":"markdown","metadata":{"id":"N_N_DtkXLmOE"},"source":["Causal Impact Library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmHE4gr-Kd5_"},"outputs":[],"source":["def causalimpact_corrs (x, y) :\n","  df = pd.DataFrame({'x': x, 'y': y})\n","  return {\n","      'Pearson': df.corr().loc['x', 'y'],\n","  }"]},{"cell_type":"markdown","metadata":{"id":"T5FFw261yx8g"},"source":["Pytorch Library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfGe4g4my5ib"},"outputs":[],"source":["def pytorch_corrs(x, y):\n","    x_tensor = torch.tensor(x)\n","    y_tensor = torch.tensor(y)\n","    cov_xy = ((x_tensor - x_tensor.mean()) * (y_tensor - y_tensor.mean())).mean()\n","    std_x = x_tensor.std()\n","    std_y = y_tensor.std()\n","\n","    return {\n","        'Pearson': (cov_xy / (std_x * std_y)).item(),\n","    }"]},{"cell_type":"markdown","metadata":{"id":"fwwhkpxp6NA-"},"source":["Dask Array Library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1dj3lQhn6MJQ"},"outputs":[],"source":["# implements a subset of the NumPy ndarray interface using blocked algorithms, cutting up the large array into many small arrays.\n","def dask_coors(x, y):\n","    x_dask = da.from_array(x, chunks = 50)\n","    y_dask = da.from_array(y, chunks = 50)\n","    return {\n","      'Pearson': (((x_dask - x_dask.mean().compute()) * (y_dask - y_dask.mean().compute())).mean().compute()/ (x_dask.std().compute()*y_dask.std().compute())),\n","    }"]},{"cell_type":"markdown","metadata":{"id":"1GoAkXIk66Gm"},"source":["Keras and Tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nMh0oHrw7j88"},"outputs":[],"source":["def keras_corrs (x, y):\n","    x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n","    y_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n","\n","    # Compute Pearson's correlation coefficient\n","    mean_x = tf.reduce_mean(x_tensor)\n","    mean_y = tf.reduce_mean(y_tensor)\n","    std_x = tf.math.reduce_std(x_tensor)\n","    std_y = tf.math.reduce_std(y_tensor)\n","\n","    # Compute the covariance\n","    covariance = tf.reduce_mean((x_tensor - mean_x) * (y_tensor - mean_y))\n","\n","    # Pearson correlation coefficient\n","    pearson_corr = covariance / (std_x * std_y)\n","\n","    return {\n","      'Pearson':  (pearson_corr).numpy(),\n","    }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12837,"status":"ok","timestamp":1725564452845,"user":{"displayName":"Ghazal Mousavi","userId":"18391534113529090965"},"user_tz":-210},"id":"d7LtRuJ4LSE4","outputId":"45496608-b0e2-4fdb-f328-6092b6b0d809"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/cluster/_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and continuous values for target\n","  warnings.warn(msg, UserWarning)\n"]}],"source":["results = {\n","    'Scipy': scipy_corrs(x, y, binary_array),\n","    'Sklearn': sklearn_corrs(x, y),\n","    'Pandas': pandas_corrs(x, y, binary_array),\n","    'Numpy': numpy_corrs(x, y),\n","    'Statmodels': statsmodels_corrs(x, y, binary_array),\n","    'Pingouin':pingouin_corrs(x, y),\n","    'Dcor': dcor_corrs(x, y),\n","    'Vaex': vaex_corrs(x, y),\n","    'Xarray': xarray_corrs(x, y),\n","    'Spark': spark_corrs(x, y),\n","    'Geopandas': geopandas_corrs(x, y),\n","    'CausalImpact': causalimpact_corrs(x, y),\n","    'Pytorch': pytorch_corrs(x, y),\n","    'Dask': dask_coors(x, y),\n","    'Keras': keras_corrs(x, y)\n","\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":637,"status":"ok","timestamp":1725564454843,"user":{"displayName":"Ghazal Mousavi","userId":"18391534113529090965"},"user_tz":-210},"id":"kEYjy53TS-h_","outputId":"7e98a325-52e8-4d72-c844-3603c5f6ee66"},"outputs":[],"source":["results_df = pd.DataFrame(results).T\n","\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_colwidth', None)\n","\n","\n","results_df.index.name = 'Library'\n","results_df.reset_index(inplace = True)\n","results_df = results_df.rename_axis(None, axis = 1)\n","pd.set_option('display.float_format', '{:.20f}'.format)\n","\n","print(results_df)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
