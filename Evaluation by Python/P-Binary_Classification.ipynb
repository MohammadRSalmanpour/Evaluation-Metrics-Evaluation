{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Nv2W-TP_x3S8"
      },
      "outputs": [],
      "source": [
        "!pip install pycm\n",
        "!pip install torchmetrics\n",
        "!pip install scipy\n",
        "!pip install statsmodels\n",
        "!pip install nltk\n",
        "!pip install SimpleITK\n",
        "!pip install evaluate\n",
        "!pip install mlxtend\n",
        "!pip install Pattern\n",
        "!pip install ignite\n",
        "!pip install pytorch-ignite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAwJmD-kdXJG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from decimal import getcontext, Decimal\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QXZdRdXcoR8v"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0BtN1HTpDCK"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Loss Function Evaluation/Data/Classification/Binary/\n",
        "%cd /content/drive/MyDrive/Data/Classification/Binary/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFCoYf9duA-Z"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npFY00FAvfnr"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=25)\n",
        "getcontext().prec = 25\n",
        "pd.options.display.float_format = '{:.25f}'.format\n",
        "np.set_printoptions(precision=25)\n",
        "pd.set_option('display.float_format', '{:.25f}'.format)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNpoMFR3lgPJ"
      },
      "source": [
        "**Data 1: US MRI Radiomecs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XqAGJvUiya3d"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "mri_features = pd.read_excel('MRI_Radiomics Features.xlsx')\n",
        "us_mri_features = pd.read_excel('US_MRI_Radiomics Features.xlsx')\n",
        "ucla_data = pd.read_excel('UCLA.xlsx')\n",
        "\n",
        "print(f'MRI_Features: {mri_features.shape}')\n",
        "print(f'us_mri_features: {us_mri_features.shape}')\n",
        "print(f'ucla_data: {ucla_data.shape}')\n",
        "\n",
        "# Update lesion names in ucla_data\n",
        "ucla_data['lesion_name'] = ucla_data['lesion_name'].str.replace('_US_Prostate', '')\n",
        "\n",
        "# Merge data based on lesion_name and PatientID\n",
        "merged_data = ucla_data.merge(mri_features, left_on='lesion_name', right_on='PatientID')\n",
        "\n",
        "\n",
        "# Remove a specific class (for example, class 1) from multi-class data\n",
        "class_to_remove = 1\n",
        "filtered_data = merged_data[merged_data['UCLA Score (Similar to PIRADS v2)_Multi-class'] != 0]\n",
        "filtered_data = filtered_data[filtered_data['UCLA Score (Similar to PIRADS v2)_Multi-class'] != 1]\n",
        "filtered_data = filtered_data[filtered_data['UCLA Score (Similar to PIRADS v2)_Multi-class'] != 2]\n",
        "\n",
        "\n",
        "# Drop name columns and separate features and target variables\n",
        "features_filtered = filtered_data.drop(columns=['lesion_name', 'PatientID', 'Labels_Binary', 'Label'])\n",
        "y_true_bi_filtered = filtered_data['Labels_Binary']\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_filtered, y_true_bi_filtered, test_size=0.2, random_state=42)\n",
        "y_true_1 = y_test\n",
        "y_true_1 = np.array(y_true_1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Model\n",
        "model_rf = RandomForestClassifier(random_state=42)\n",
        "model_rf.fit(X_train, y_train)\n",
        "y_pred_1 = model_rf.predict(X_test)\n",
        "y_pred_proba_1 = model_rf.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkqBOQb3mNo9"
      },
      "source": [
        "**Data 2: Random Data:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfZLdMou1Igv"
      },
      "outputs": [],
      "source": [
        "data_multi = pd.read_csv('/content/drive/MyDrive/Loss Function Evaluation/Data/Classification/Others/y_true_y_pred_binary.csv')\n",
        "y_true_2 = data_multi['y_true']\n",
        "y_pred_2 = data_multi['y_pred']\n",
        "y_true_2 = np.array(y_true_2)\n",
        "y_pred_2 = np.array(y_pred_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "dv4SbtW8l4nH",
        "outputId": "3d98c27f-58f7-4b1a-f8ef-9c329f518eaf"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from openpyxl import Workbook\n",
        "\n",
        "# Function to process each dataset\n",
        "def process_dataset(features, targets):\n",
        "    features.columns = features.columns.astype(str)\n",
        "\n",
        "    # Preprocess the data\n",
        "    X = features.iloc[:, 1:]\n",
        "    y = targets.iloc[:, -1]\n",
        "        # Drop rows with NaN values in the target\n",
        "    X = X[~y.isna()]\n",
        "    y = y.dropna()\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    model = RandomForestClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict the labels\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)\n",
        "    y_pred_proba = y_pred_proba[:, 1]  # Adjust index if necessary\n",
        "\n",
        "\n",
        "    # Return true and predicted labels as a DataFrame\n",
        "    return pd.DataFrame({'y_true': y_test, 'y_pred': y_pred, 'y_pred_proba': y_pred_proba})\n",
        "\n",
        "# Load the datasets (features and targets)\n",
        "features_df1 = pd.read_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Classification/Binary/HC_DF_SPT_Head and Neck.xlsx', sheet_name='feature')\n",
        "targets_df1 = pd.read_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Classification/Binary/HC_DF_SPT_Head and Neck.xlsx', sheet_name='target')\n",
        "\n",
        "features_df2 = pd.read_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Classification/Binary/HC_RF_CT_Head and Neck.xlsx', sheet_name='feature')\n",
        "targets_df2 = pd.read_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Classification/Binary/HC_RF_CT_Head and Neck.xlsx', sheet_name='target')\n",
        "\n",
        "features_df3 = pd.read_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Classification/Binary/HC_RF_PT_Head and Neck.xlsx', sheet_name='feature')\n",
        "targets_df3 = pd.read_excel('/content/drive/MyDrive/Loss Function Evaluation/Data/Classification/Binary/HC_RF_PT_Head and Neck.xlsx', sheet_name='target')\n",
        "\n",
        "# Process each dataset separately\n",
        "results_df1 = process_dataset(features_df1, targets_df1)\n",
        "results_df2 = process_dataset(features_df2, targets_df2)\n",
        "results_df3 = process_dataset(features_df3, targets_df3)\n",
        "\n",
        "# Save all results to a single Excel file with separate sheets\n",
        "with pd.ExcelWriter('classification_results.xlsx') as writer:\n",
        "    results_df1.to_excel(writer, sheet_name='HC_DF_SPT_Head and Neck', index=False)\n",
        "    results_df2.to_excel(writer, sheet_name='HC_RF_CT_Head and Neck', index=False)\n",
        "    results_df3.to_excel(writer, sheet_name='HC_RF_PT_Head and Neck', index=False)\n",
        "\n",
        "# Save all results to a specific directory\n",
        "with pd.ExcelWriter('/content/drive/MyDrive/Loss Function Evaluation/Data/Classification/Binary/classification_results.xlsx') as writer:\n",
        "    results_df1.to_excel(writer, sheet_name='HC_DF_SPT_Head and Neck', index=False)\n",
        "    results_df2.to_excel(writer, sheet_name='HC_RF_CT_Head and Neck', index=False)\n",
        "    results_df3.to_excel(writer, sheet_name='HC_RF_PT_Head and Neck', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efSt3ZfydSCt"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "df = pd.read_excel('classification_results.xlsx', sheet_name='HC_DF_SPT_Head and Neck')\n",
        "y_true = df['y_true'].values\n",
        "y_pred = df['y_pred'].values\n",
        "y_pred_proba = df[['y_pred_proba']].values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4eyxZLnxC-D"
      },
      "outputs": [],
      "source": [
        "y_true =y_true_2\n",
        "y_pred =  y_pred_2\n",
        "#y_pred_proba = y_pred_proba_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH0nQPp-mrDu"
      },
      "source": [
        "**Scikit Learn Library:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCZN3XXZmv0o"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (classification_report, accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             log_loss, cohen_kappa_score, roc_curve, auc, matthews_corrcoef,\n",
        "                             balanced_accuracy_score, confusion_matrix, jaccard_score, fbeta_score)\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "results['Scikit-learn'] = {\n",
        "    'accuracy': accuracy_score(y_true, y_pred),\n",
        "    'precision': precision_score(y_true, y_pred),\n",
        "    'recall': recall_score(y_true, y_pred),\n",
        "    'f1': f1_score(y_true, y_pred),\n",
        "    'log_loss': log_loss(y_true, y_pred_proba),\n",
        "    'kappa': cohen_kappa_score(y_true, y_pred),\n",
        "    'mcc': matthews_corrcoef(y_true, y_pred),\n",
        "    'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
        "    'f beta_score': fbeta_score(y_true, y_pred, beta=0.5),\n",
        "    'jaccard_index': jaccard_score(y_true, y_pred),\n",
        "    'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
        "    'AUC': roc_auc\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZThdnLc8m3Ol"
      },
      "source": [
        "**Pycm Library:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsR3btPEm6fh"
      },
      "outputs": [],
      "source": [
        "from pycm import ConfusionMatrix\n",
        "\n",
        "y_pred = np.array(y_pred)  # Convert to NumPy array if not already\n",
        "y_true = np.array(y_true)\n",
        "\n",
        "cm = ConfusionMatrix(actual_vector=y_true, predict_vector=y_pred)\n",
        "results['PyCM'] = {\n",
        "    'accuracy': cm.Overall_ACC,\n",
        "    'precision': cm.PPV[1],\n",
        "    'recall': cm.TPR[1],\n",
        "    'f1': cm.F1[1],\n",
        "    'kappa': cm.Kappa,\n",
        "    'mcc': cm.Overall_MCC,\n",
        "    'jaccard_index': cm.J[1],\n",
        "    'confusion_matrix': cm.to_array()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXj_wq-TnBAK"
      },
      "source": [
        "**Tensorflow Library:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5QsF1WpnIsi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "y_true_tensor = tf.constant(y_true, dtype=tf.float32)\n",
        "y_pred_tensor = tf.constant(y_pred, dtype=tf.float32)\n",
        "y_pred_prob_tensor = tf.constant(y_pred_proba, dtype=tf.float32)\n",
        "\n",
        "# Reshape y_true_tensor and y_pred_tensor to 2D shape\n",
        "y_true_tensor = tf.reshape(y_true_tensor, [-1, 1])\n",
        "y_pred_tensor = tf.reshape(y_pred_tensor, [-1, 1])\n",
        "y_prob_tensor = tf.reshape(y_pred_prob_tensor, [-1, 1])\n",
        "\n",
        "\n",
        "# Define metrics\n",
        "accuracy = tf.keras.metrics.Accuracy()\n",
        "precision = tf.keras.metrics.Precision()\n",
        "recall_macro = tf.keras.metrics.Recall()\n",
        "f1 = tf.keras.metrics.F1Score(average='weighted')  # Adjust num_classes and average as needed\n",
        "fbeta = tf.keras.metrics.FBetaScore( beta=0.5, average=None)  # Adjust num_classes and beta as needed\n",
        "jaccard = tf.keras.metrics.BinaryIoU(target_class_ids=[0, 1])\n",
        "log_loss = tf.keras.metrics.BinaryCrossentropy(from_logits=False)\n",
        "\n",
        "\n",
        "# Update metrics with your data\n",
        "accuracy.update_state(y_true_tensor, y_pred_tensor)\n",
        "precision.update_state(y_true_tensor, y_pred_tensor)\n",
        "recall_macro.update_state(y_true_tensor, y_pred_tensor)\n",
        "log_loss.update_state(y_true_tensor, y_prob_tensor)\n",
        "f1.update_state(y_true_tensor, y_pred_tensor)\n",
        "fbeta.update_state(y_true_tensor, y_pred_tensor)\n",
        "jaccard.update_state(y_true_tensor, y_pred_tensor)\n",
        "auc = tf.keras.metrics.AUC()\n",
        "auc.update_state(y_true, y_pred)\n",
        "\n",
        "results['TensorFlow'] = {\n",
        "    'accuracy': accuracy.result().numpy(),\n",
        "    'precision': precision.result().numpy(),\n",
        "    'recall': recall_macro.result().numpy(),\n",
        "    'f1': f1.result().numpy(),\n",
        "    'log_loss': log_loss.result().numpy(),\n",
        "    'f beta_score': fbeta.result().numpy(),\n",
        "    'jaccard_index': jaccard.result().numpy(),\n",
        "    'AUC': auc.result().numpy()\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAKdawbOnU3u"
      },
      "source": [
        "**Evaluate Library:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Go1M46cnfBl"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy_evaluate = evaluate.load(\"accuracy\")\n",
        "precision_evaluate = evaluate.load(\"precision\")\n",
        "recall_evaluate = evaluate.load(\"recall\")\n",
        "f1_evaluate = evaluate.load(\"f1\")\n",
        "mcc_evaluate = evaluate.load(\"matthews_correlation\")\n",
        "confusion_matrix_evaluate = evaluate.load(\"confusion_matrix\")\n",
        "\n",
        "\n",
        "accuracy_result = accuracy_evaluate.compute(references=y_true, predictions=y_pred)\n",
        "precision_result = precision_evaluate.compute(references=y_true, predictions=y_pred)\n",
        "recall_result = recall_evaluate.compute(references=y_true, predictions=y_pred)\n",
        "f1_result = f1_evaluate.compute(references=y_true, predictions=y_pred)\n",
        "mcc_result = mcc_evaluate.compute(references=y_true, predictions=y_pred)\n",
        "confusion_matrix_result = confusion_matrix_evaluate.compute(references=y_true, predictions=y_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "results['evaluate'] = {\n",
        "    'accuracy': accuracy_result['accuracy'],\n",
        "    'precision': precision_result['precision'],\n",
        "    'recall': recall_result['recall'],\n",
        "    'f1': f1_result['f1'],\n",
        "    'mcc': mcc_result['matthews_correlation'],\n",
        "    'confusion_matrix': confusion_matrix_result['confusion_matrix'],\n",
        "    #\"AUC\" : auc['roc_auc']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubVyt9dsnqBt"
      },
      "source": [
        "**Torch Metrics Library:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc3q3Qnwn3ZB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchmetrics import (Accuracy, Precision, Recall, F1Score, MatthewsCorrCoef, CohenKappa,\n",
        "                          FBetaScore, JaccardIndex, ConfusionMatrix)\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics.classification import BinaryAUROC\n",
        "# Convert your true and predicted values to PyTorch tensors\n",
        "y_true_tensor = torch.tensor(y_true)\n",
        "y_pred_tensor = torch.tensor(y_pred)\n",
        "\n",
        "\n",
        "# Define the metrics\n",
        "accuracy = Accuracy('binary')\n",
        "precision = Precision('binary')\n",
        "recall = Recall('binary')\n",
        "f1_score = F1Score('binary')\n",
        "matthews_corrcoef = MatthewsCorrCoef('binary')\n",
        "cohen_kappa = CohenKappa('binary')\n",
        "fbeta_score = FBetaScore('binary',beta=0.5)\n",
        "jaccard_index = JaccardIndex('binary')\n",
        "confusion_matrix = ConfusionMatrix('binary')\n",
        "\n",
        "\n",
        "auroc = BinaryAUROC()\n",
        "auc_value = auroc(y_pred_tensor, y_true_tensor)\n",
        "\n",
        "\n",
        "# Compute the metrics using TorchMetrics\n",
        "results['TorchMetrics'] = {\n",
        "    'accuracy': accuracy(y_pred_tensor, y_true_tensor).item(),\n",
        "    'precision': precision(y_pred_tensor, y_true_tensor).item(),\n",
        "    'recall': recall(y_pred_tensor, y_true_tensor).item(),\n",
        "    'f1': f1_score(y_pred_tensor, y_true_tensor).item(),\n",
        "    'mcc': matthews_corrcoef(y_pred_tensor, y_true_tensor).item(),\n",
        "    'kappa': cohen_kappa(y_pred_tensor, y_true_tensor).item(),\n",
        "    'f beta_score': fbeta_score(y_pred_tensor, y_true_tensor).item(),\n",
        "    'jaccard_index': jaccard_index(y_pred_tensor, y_true_tensor).item(),\n",
        "    'confusion_matrix': confusion_matrix(y_pred_tensor, y_true_tensor).numpy(),\n",
        "    'AUC':auc_value.item()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac3cv8Uan9mw"
      },
      "source": [
        "**Mlxtend Library:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XaA5vZ7oE2S"
      },
      "outputs": [],
      "source": [
        "from mlxtend.evaluate import confusion_matrix as mlxtend_confusion_matrix, accuracy_score as mlxtend_accuracy_score\n",
        "\n",
        "results['MLxtend'] = {\n",
        "    'accuracy': mlxtend_accuracy_score(y_true, y_pred),\n",
        "    'confusion_matrix': mlxtend_confusion_matrix(y_true, y_pred)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlsx6cyFoLUF"
      },
      "source": [
        "**nlkt Library:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89zIqrP6oRki"
      },
      "outputs": [],
      "source": [
        "from nltk.metrics import accuracy, precision, recall, f_measure, ConfusionMatrix\n",
        "\n",
        "def multi_class_metrics(y_true, y_pred, labels):\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f_measures = []\n",
        "\n",
        "    for label in labels:\n",
        "        reference_set = set((i for i, y in enumerate(y_true) if y == label))\n",
        "        test_set = set((i for i, y in enumerate(y_pred) if y == label))\n",
        "\n",
        "        precisions.append(precision(reference_set, test_set))\n",
        "        recalls.append(recall(reference_set, test_set))\n",
        "        f_measures.append(f_measure(reference_set, test_set))\n",
        "\n",
        "    avg_precision = sum(p for p in precisions if p is not None) / len(labels)\n",
        "    avg_recall = sum(r for r in recalls if r is not None) / len(labels)\n",
        "    avg_f_measure = sum(f for f in f_measures if f is not None) / len(labels)\n",
        "\n",
        "    return avg_precision, avg_recall, avg_f_measure\n",
        "\n",
        "results['nltk'] = {\n",
        "    'accuracy': accuracy(y_true, y_pred),\n",
        "    'precision': precision(set(i for i, y in enumerate(y_true) if y == 1), set(i for i, y in enumerate(y_pred) if y == 1)),\n",
        "    'recall': recall(set(i for i, y in enumerate(y_true) if y == 1), set(i for i, y in enumerate(y_pred) if y == 1)),\n",
        "    'f1': f_measure(set(i for i, y in enumerate(y_true) if y == 1), set(i for i, y in enumerate(y_pred) if y == 1)),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flez6iD1oVjG"
      },
      "source": [
        "**Imbalanced-Learn Library:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLgE4urRoh4c"
      },
      "outputs": [],
      "source": [
        "from imblearn.metrics import geometric_mean_score\n",
        "\n",
        "results['Imbalanced-learn'] = {\n",
        "    'geometric_mean': geometric_mean_score(y_true, y_pred)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3eid0WColvb"
      },
      "source": [
        "**Ignite Library:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvPb8eX0o-00"
      },
      "outputs": [],
      "source": [
        "from ignite.metrics import (Accuracy as IgniteAccuracy, Precision as IgnitePrecision, Recall as IgniteRecall,\n",
        "                            Fbeta as IgniteFbeta, ConfusionMatrix as IgniteConfusionMatrix, CohenKappa as IgniteCohenKappa,\n",
        "                            Loss as IgniteLoss)\n",
        "from torch.nn.functional import binary_cross_entropy\n",
        "from ignite.metrics import Loss\n",
        "from ignite.metrics import ROC_AUC\n",
        "\n",
        "ignite_accuracy = IgniteAccuracy()\n",
        "ignite_precision = IgnitePrecision()\n",
        "ignite_recall = IgniteRecall()\n",
        "ignite_fbeta = IgniteFbeta(beta=0.5)\n",
        "ignite_f1 = IgniteFbeta(beta=1.0) # F1 score is equivalent to Fbeta with beta=1.0\n",
        "ignite_kappa = IgniteCohenKappa()\n",
        "ignite_conf_matrix = IgniteConfusionMatrix(num_classes=2)\n",
        "\n",
        "y_true_tensor_ignite = torch.tensor(y_true, dtype=torch.float32)\n",
        "y_pred_tensor_ignite = torch.tensor(y_pred, dtype=torch.float32)\n",
        "y_prob_tensor_ignite = torch.tensor(y_pred_proba, dtype=torch.float32)\n",
        "\n",
        "# Define the binary cross-entropy loss function\n",
        "ignite_loss = Loss(F.binary_cross_entropy)\n",
        "\n",
        "y_prob_tensor_ignite = y_prob_tensor_ignite.squeeze()\n",
        "\n",
        "from ignite.metrics import ROC_AUC\n",
        "\n",
        "auc_metric = ROC_AUC()\n",
        "auc_metric.update((y_pred_tensor_ignite, y_true_tensor_ignite))\n",
        "auc_value = auc_metric.compute()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ignite_accuracy.update((y_pred_tensor_ignite, y_true_tensor_ignite))\n",
        "ignite_precision.update((y_pred_tensor_ignite, y_true_tensor_ignite))\n",
        "ignite_recall.update((y_pred_tensor_ignite, y_true_tensor_ignite))\n",
        "ignite_fbeta.update((y_pred_tensor_ignite, y_true_tensor_ignite))\n",
        "ignite_f1.update((y_pred_tensor_ignite, y_true_tensor_ignite))\n",
        "ignite_kappa.update((y_pred_tensor_ignite, y_true_tensor_ignite))\n",
        "ignite_loss.update((y_prob_tensor_ignite, y_true_tensor_ignite))\n",
        "\n",
        "\n",
        "results['Ignite'] = {\n",
        "    'accuracy': ignite_accuracy.compute(),\n",
        "    'precision': ignite_precision.compute().item(),\n",
        "    'recall': ignite_recall.compute().item(),\n",
        "    'f1': ignite_f1.compute(),  # F1 score\n",
        "    'f beta_score': ignite_fbeta.compute(),\n",
        "    'kappa': ignite_kappa.compute(),\n",
        "    'log_loss':  ignite_loss.compute(),\n",
        "    'AUC' : auc_value\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J70-76xYbcdU",
        "outputId": "0d487504-b8f6-478a-dc43-ea23c7b3730c"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df.index.name = 'Library'  \n",
        "results_df.reset_index(inplace=True)  \n",
        "results_df = results_df.rename_axis(None, axis=1)  \n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8isL-Gaw8WT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
